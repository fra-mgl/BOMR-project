{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire Truck Project Report\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Vision Module](#vision-module)\n",
    "   - 2.1 [Overview](#overview)\n",
    "   - 2.2 [Reading the Predefined Grid](#reading-the-predefined-grid)\n",
    "   - 2.3 [Feature Extraction](#feature-extraction)\n",
    "   - 2.4 [QR Code Identification](#qr-code-identification)\n",
    "   - 2.5 [Objectives](#objectives)\n",
    "      - 2.5.1 [Determining the Robot's Position](#determining-the-robots-position)\n",
    "      - 2.5.2 [Recognizing Obstacles (In Red)](#recognizing-obstacles)\n",
    "      - 2.5.3 [Identifying the Goal (In Blue)](#identifying-the-goal)\n",
    "   - 2.6 [Pre-processing for Vision](#pre-processing)\n",
    "   - 2.7 [Mapping and Localization](#mapping-and-localization)\n",
    "\n",
    "3. [Optimal Path Computation](#optimal-path-computation)\n",
    "   - 3.1 [Pathfinding Precision](#pathfinding-precision)\n",
    "   - 3.2 [A* Algorithmic Mastery](#a-algorithmic-mastery)\n",
    "   - 3.3 [Navigational Options](#navigational-options)\n",
    "   - 3.4 [Path Reconstruction Elegance](#path-reconstruction-elegance)\n",
    "\n",
    "4. [Motion Control Implementation](#motion-control-implementation)\n",
    "   - 4.1 [Initialization and Configuration](#initialization-and-configuration)\n",
    "   - 4.2 [Sensor Data Retrieval and Actuation](#sensor-data-retrieval-and-actuation)\n",
    "   - 4.3 [Trajectory Following Mechanism](#trajectory-following-mechanism)\n",
    "\n",
    "5. [Local Navigation Strategy](#local-navigation-strategy)\n",
    "   - 5.1 [Obstacle Detection and Handling](#obstacle-detection-and-handling)\n",
    "   - 5.2 [Maneuvering Strategies](#maneuvering-strategies)\n",
    "\n",
    "6. [Kalman Filter in Localization](#kalman-filter-in-localization)\n",
    "   - 6.1 [Adaptive Position Estimation](#adaptive-position-estimation)\n",
    "   - 6.2 [Compensating for Uncertainties](#compensating-for-uncertainties)\n",
    "\n",
    "\n",
    "## **1. Introduction<a name=\"introduction\"></a>**\n",
    "\n",
    "This comprehensive study highlights the complexities of our robotics research as we set out to replicate the functions of an autonomous fire rescue truck. The main goal is to create and deploy a robotic system with advanced features that is comparable to what is needed in a fire rescue situation. A sophisticated filtering unit that facilitates precise position estimation, real-time position tracking that ensures dynamic situational awareness, global navigation for strategic pathfinding, local navigation for on-the-fly obstacle avoidance, and a responsive control system that orchestrates the robot's movements in a fire emergency context are among the key components.\n",
    "\n",
    "Our project's story is based on a simulated scenario in which an autonomous fire rescue robot sets out on a mission to locate and rescue people trapped in a burning building, taking inspiration from the urgent and essential nature of firefighting scenarios. This hypothetical scenario highlights the necessity for robotic systems to manoeuvre through dangerous situations, assess the situation, and carry out precision rescue operations. It also matches the urgency and complexity of actual firefighting operations where speed and efficiency are keys for a successful salvation of the endangered victims.\n",
    "\n",
    "### **Environment**\n",
    "\n",
    "The operational canvas of our robot unfolds across a dynamic terrain represented by a grid-type map. Comprising 49 squares (7 rows and 7 columns), the map features white pathways and black barriers. ArUco markers strategically designate corners, the goal, and the robot's position as well as obstcales. This environment facilitates the interplay of the vision module and the Thymio robot, allowing it to follow the global navigation path and dynamically adapt to obstacles using local navigation. A Kalman filter implementation further augments precision, incorporating equations for x and y coordinates, Thymio's angle, and motor velocities.\n",
    "\n",
    "### **Structure**\n",
    "\n",
    "In our project's organisational structure, a central \"Command Centre\" assumes a pivotal role as the hub for coordinating the cooperation of many modules, ensuring the seamless functioning of our self-contained fire rescue robot. This crucial module takes on the tasks of establishing the Thymio connection, carrying out careful camera calibration to maximise colour and edge detection, calculating the grid map, and drawing the route for navigation. This command centre is modular in operation and handles critical robot duties such as taking quick decisions regarding the presence and absence of an obstacle. The robot operates in two different modes: local navigation, which allows for real-time obstacle avoidance, and global navigation, which plots a route to the closest checkpoint. While maintaining knowledge of the position and orientation of the robot using either the Camera module or the Kalman filter.\n",
    "\n",
    "### **Modules**\n",
    "\n",
    "#### **Robot State Estimation with Extended Kalman Filter**\n",
    "\n",
    "In our autonomous fire rescue robot project, accurate position and speed estimation are vital for effective navigation. We utilize an Extended Kalman Filter (EKF) implemented in the `KalmanFilter` class. The state space representation is tailored to our specific needs, considering a 4-dimensional state vector \\([ \\text{x}, \\text{y}, \\text{v}_x, \\text{v}_y ]\\), where \\(\\text{v}_x\\) and \\(\\text{v}_y\\) represent the linear velocities in the x and y directions, respectively.\n",
    "\n",
    "\n",
    "##### **Coordinate System**\n",
    "\n",
    "The coordinate system follows the convention shown below:\n",
    "\n",
    "Here, \\(x\\) and \\(y\\) are in millimeters, and \\(\\text{{vx}}, \\text{{vy}}\\) are in millimeters per second.\n",
    "\n",
    "##### **Discrete Model**\n",
    "\n",
    "The discrete motion model for the Kalman filter is given by:\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\mathbf{z}_{k+1} &= f(\\mathbf{z}_k) \\\\\n",
    "\\mathbf{z}^+ &= \\begin{bmatrix} x + \\frac{v_r + v_l}{2} \\cos(\\theta) T_s \\\\ y - \\frac{v_r + v_l}{2} \\sin(\\theta) T_s \\\\ \\text{{vx}} \\\\ \\text{{vy}} \\end{bmatrix} + \\mathbf{w}\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "Here, \\(k_s\\) is the constant factor, and \\(T_s\\) is the time between two iterations. The covariance matrix \\(\\mathbf{Q}\\) represents the motion model's uncertainty.\n",
    "\n",
    "##### **Measurement Model**\n",
    "\n",
    "The discrete measurement model is given by:\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\mathbf{z}_{k+1} &= h(\\mathbf{z}_k) \\\\\n",
    "\\mathbf{z}^+ &= \\begin{bmatrix} x_{\\text{{measured}}} \\\\ y_{\\text{{measured}}} \\\\ v_{r \\text{{ measured}}} \\\\ v_{l \\text{{ measured}}} \\end{bmatrix} + \\mathbf{v}\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "The measurement model incorporates data from the camera, providing measured values for position, orientation, and motor speeds. The covariance matrix \\(\\mathbf{R}\\) captures the measurement noise.\n",
    "\n",
    "##### **Extended Kalman Filter Algorithm**\n",
    "\n",
    "The steps of the extended Kalman filter algorithm, tailored to our implementation, are as follows:\n",
    "\n",
    "1. **Prediction Step:**\n",
    "   \\begin{align*}\n",
    "   \\hat{\\mathbf{x}}_{k \\mid k-1} &= f(\\hat{\\mathbf{x}}_{k-1 \\mid k-1}, \\mathbf{u}_k) \\\\\n",
    "   \\mathbf{P}_{k \\mid k-1} &= \\mathbf{F}_k \\mathbf{P}_{k-1 \\mid k-1} \\mathbf{F}_k^T + \\mathbf{Q}_k\n",
    "   \\end{align*}\n",
    "\n",
    "2. **Measurement Residual:**\n",
    "   \\begin{align*}\n",
    "   \\tilde{\\mathbf{y}}_k &= \\mathbf{z}_k - h(\\hat{\\mathbf{x}}_{k \\mid k-1})\n",
    "   \\end{align*}\n",
    "\n",
    "3. **Measurement Update:**\n",
    "   \\begin{align*}\n",
    "   \\mathbf{S}_k &= \\mathbf{H}_k \\mathbf{P}_{k \\mid k-1} \\mathbf{H}_k^T + \\mathbf{R}_k \\\\\n",
    "   \\mathbf{K}_k &= \\mathbf{P}_{k \\mid k-1} \\mathbf{H}_k^T (\\mathbf{S}_k)^{-1} \\\\\n",
    "   \\hat{\\mathbf{x}}_{k \\mid k} &= \\hat{\\mathbf{x}}_{k \\mid k-1} + \\mathbf{K}_k \\tilde{\\mathbf{y}}_k \\\\\n",
    "   \\mathbf{P}_{k \\mid k} &= (\\mathbf{I} - \\mathbf{K}_k \\mathbf{H}_k) \\mathbf{P}_{k \\mid k-1}\n",
    "   \\end{align*}\n",
    "\n",
    "\n",
    "##### **Estimation of Ks and Ts**\n",
    "\n",
    "The time between iterations (\\(T_s\\)) is determined by measuring the elapsed time. The estimation of \\(k_s\\) involves observing the robot's rotation for about 50 seconds, counting the degrees turned, and calculating a coefficient, resulting in \\(9.20 \\times 10^{-3}\\).\n",
    "\n",
    "##### **Estimation of Noise Matrices**\n",
    "\n",
    "For the motion model:<br />\n",
    "$$\n",
    "Q=\\begin{bmatrix}\n",
    "0.04 & 0 & 0 & 0 & 0\\\\\n",
    "& 0.04 & 0 & 0 & 0\\\\\n",
    "& & 0.02 & 0 & 0\\\\\n",
    "& & & 6.153 & 0\\\\\n",
    "& & & & 6.153\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We estimated 0.04, which is estimated arbitrarily, the same for the angle. For speed, we used our measurements from week 8, so half of the variance came from the motion and the other half from the measurement. <br>\n",
    "\n",
    "For the measurement model:<br />\n",
    "$$\n",
    "R=\\begin{bmatrix}\n",
    "1.798 & 0 & 0 & 0 & 0\\\\\n",
    "0 & 1.798 & 0 & 0 & 0\\\\\n",
    "0 & 0 & 0.002 & 0 & 0\\\\\n",
    "0 & 0 & 0 & 6.153 & 0\\\\\n",
    "0 & 0 & 0 & 0 & 6.153\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These values are estimated, with half of the variance attributed to motion and the other half to measurement.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 2. Vision Module<a name=\"vision-module\"></a>\n",
    "\n",
    "### 2.1 Overview<a name=\"overview\"></a>\n",
    "\n",
    "The vision module serves as the visual intelligence of the Thymio robot, forming a critical component within the broader project framework. Its primary purpose revolves around interpreting the surrounding environment through a predefined grid, feature extraction, and the recognition of specific visual cues, such as QR codes. The overarching objectives of this module are multi-faceted and include determining the robot's precise position, identifying obstacles marked in red, pinpointing the goal represented in blue, and detecting a secondary goal highlighted in green.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from math import floor\n",
    "from enum import Enum\n",
    "from vision.utils import print_error\n",
    "from vision.constants import grid_height_cells, grid_width_cells\n",
    "```\n",
    "\n",
    "```python\n",
    "    class VisionObjectsTypes(Enum):\n",
    "        GENERAL = 0\n",
    "        OBSTACLE = 1\n",
    "        GOAL = 2\n",
    "        TARGET = 3\n",
    "```\n",
    "\n",
    "The vision module starts by importing necessary libraries for image processing, numerical operations, and custom utility functions and constants needed for this project. The enumeration class presented above defines different types of vision objects: general, obstacle, goal, and target.\n",
    "\n",
    "### 2.2 Reading the Predefined Grid<a name=\"reading-the-predefined-grid\"></a>\n",
    "\n",
    "The core functionality of the vision module lies in its ability to comprehend a predefined grid. This involves processing visual data to interpret the grid's layout, which serves as the navigational reference for the Thymio robot. By parsing the grid, the vision module equips the robot with essential spatial awareness, enabling it to make informed decisions about its movements and thus allowing more complex control of the thymio such as the A* global path computation or the P controller for the robot's movements.\n",
    "\n",
    "### 2.3 Feature Extraction<a name=\"feature-extraction\"></a>\n",
    "\n",
    "To navigate effectively, the vision module engages in feature extraction, isolating distinctive elements within the visual input. These features are in general the edges, corners and the unique patterns of the QR codes that aid in recognizing the environment. By extracting these features, we gain a more detailed understanding the surroundings and utilising this to precise localization and navigation.\n",
    "\n",
    "### 2.4 QR Code Identification<a name=\"qr-code-identification\"></a>\n",
    "\n",
    "The vision module is designed to identify QR codes, leveraging their encoded information for various purposes within the project such as conveying data regarding the Thymio's position, orientation and the Grid's corners.\n",
    "\n",
    "### 2.5 Objectives<a name=\"objectives\"></a>\n",
    "\n",
    "#### 2.5.1 Determining the Robot's Position<a name=\"determining-the-robots-position\"></a>\n",
    "\n",
    "A fundamental objective of the vision module is to accurately determine the Thymio robot's position within its environment. This involves integrating information from the predefined grid, extracted features, and QR codes to establish a real-time understanding of the robot's spatial coordinates, speed and orienation.\n",
    "\n",
    "#### 2.5.2 Recognizing Obstacles (In Red)<a name=\"recognizing-obstacles\"></a>\n",
    "\n",
    "The vision module is tasked with identifying obstacles marked in red. This capability enables the Thymio robot to proactively detect and respond to potential hindrances in its path, facilitating efficient and safe navigation. Using the camera to locate the predefined obstacles in red allows the robot to only detect any new detected obstacle in it's path.\n",
    "\n",
    "#### 2.5.3 Identifying the Goal (In Blue)<a name=\"identifying-the-goal\"></a>\n",
    "\n",
    "Recognizing the goal, indicated in blue, is important for the robot's navigation strategy. The vision module's ability to identify the goal allows the Thymio robot to plan and execute optimal paths using algorithms (in our case the __A*__), steering towards final destinations.\n",
    "\n",
    "In essence, the vision module serves as the \"eyes\" of the Thymio robot, enabling it to perceive, interpret, and respond intelligently to the visual cues present in its environment. The successful execution of these vision objectives forms the foundation for the robot's overall navigation and mission accomplishment.\n",
    "\n",
    "### 2.6 Pre-processing for Vision<a name=\"pre-processing\"></a>\n",
    "\n",
    "The original frames captured by the camera undergo pre-processing to address issues like chromatic aberration, noise, and unstandardization caused by changes in camera position and shooting angle. The map region is extracted using markers, and perspective projection is removed with a projective transform for robustness. The image is converted to grayscale, filtered for noise with a Gaussian filter, and thresholded for binary mapping. Future improvements may include exploring different filters or thresholding methods to enhance performance under varied conditions.\n",
    "\n",
    "### 2.7 Mapping and Localization<a name=\"mapping-and-localization\"></a>\n",
    "\n",
    "After pre-processing, the image is cropped into grids, and the vision module identifies walls and goals using ArUco markers. The robot's position and rotation gained by the vision module are used in conjunction with motor speed estimates for a more precise state estimation. This information is then utilized in control and navigation modules, providing a comprehensive understanding of the robot's real-time state.\n",
    "\n",
    "Certainly! Here is the modified section, including the provided assumptions, integrated into the local navigation part:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. Optimal Path Computation<a name=\"optimal-path-computation\"></a>\n",
    "\n",
    "### 3.1 Pathfinding Precision<a name=\"pathfinding-precision\"></a>\n",
    "\n",
    "This section will be focusing on global navigation, unraveling the choices of the Thymio's optimal path computation facilitated by the __A* algorithm__. Beyond mere movement, this algorithmic approach aspires to chart the most efficient course for the robot, navigating through a predefined grid with discernment minimising the cost needed to achieve the desired result.\n",
    "\n",
    "### 3.2 A* Algorithmic Mastery<a name=\"a-algorithmic-mastery\"></a>\n",
    "\n",
    "\n",
    "\n",
    "#### 3.3 Navigational Options<a name=\"navigational-options\"></a>\n",
    "Two distinct sets of movement options are already provided for the Thymio's trajectory within the A* algorithm. Both 4-connectivity and 8-connectivity movements are optimally computed. After careful consideration, we have chosen to use the 4N connectivity for our project. This decision is informed by the nature of the environment, which tries to mirrors a realistic scenario with streets and buildings where diagonal movements are restricted. The adoption of 4N connectivity aligns with the practical constraints of the project, ensuring computational efficiency while navigating through the constrained layout of the simulated environment.\n",
    "\n",
    "\n",
    "#### 3.4 Path Reconstruction Elegance<a name=\"path-reconstruction-elegance\"></a>\n",
    "\n",
    "The reconstruction of the path is important in how we make the Thymio move. This way of doing things in the __A* algorithm's__ code ensures that we can efficiently figure out the best path for the Thymio to follow. It's like setting up a smooth roadmap that helps the Thymio navigate carefully and reach its destination.\n",
    "\n",
    "## 4. Motion Control Implementation Overview<a name=\"motion-control-implementation-overview\"></a>\n",
    "\n",
    "### 4.1 Initialization and Configuration<a name=\"initialization-and-configuration\"></a>\n",
    "\n",
    "The `Control` class is instantiated with parameters essential for motion control, including speed conversion factors, tolerance values, and control weights. This step sets the foundational parameters for the subsequent motion control strategies. Next, we created methods for synchronous and asynchronos operations which are implemented within the `Control` class. The `sync` method ensures proper synchronization with the robot node, while the `unsync` method releases the lock and stops the robot when necessary, ensuring safe and controlled operations.\n",
    "\n",
    "\n",
    "### 4.2 Sensor Data Retrieval and Actuation<a name=\"sensor-data-retrieval-and-actuation\"></a>\n",
    "\n",
    "Methods for retrieving sensor data and adjusting motor speeds based on corrections are implemented, forming the core of the robot's adaptive motion control. These functionalities ensure that the robot responds effectively to its environment, enhancing its overall navigational capabilities.\n",
    "\n",
    "\n",
    "### 4.2 Trajectory Following Mechanism<a name=\"trajectory-following-mechanism\"></a>\n",
    "\n",
    "The trajectory-following mechanism, encapsulated in the `following_path` method, orchestrates the robot's movement towards checkpoints with dynamic orientation adjustments based on its position and predefined tolerances. If the robot nears the checkpoint, the method concludes, optimizing efficiency. Otherwise, it calculates angle adjustments, prompting either turning or forward motion to ensure precise navigation along the desired path. This adaptive approach seamlessly integrates with the motion control strategy, which employs proportional control to dynamically adjust motor speeds, optimizing the overall navigation precision and ensuring the robot's accurate movement towards checkpoints.\n",
    "\n",
    "```python\n",
    "    async def following_path(self, pos, angle, checkpoint):\n",
    "        distance = math.sqrt((pos[1] - checkpoint[1])**2 + (pos[0] - checkpoint[0])**2) \n",
    "        if distance < self.tolerance_radius:\n",
    "            return True\n",
    "        else:\n",
    "            vector_to_checkpoint = (checkpoint[0] - pos[0], checkpoint[1] - pos[1])\n",
    "            angle_to_checkpoint = math.degrees(angle_between(1,0), vector_to_checkpoint)\n",
    "            distance_to_checkpoint = math.sqrt(vector_to_checkpoint[0]**2 + vector_to_checkpoint[1]**2)\n",
    "            delta_angle = angle_to_checkpoint - angle\n",
    "            if abs(delta_angle) > self.tolerance_angle:\n",
    "                self.set_motors(self.kangle*delta_angle,\"turn\")\n",
    "            else: \n",
    "                pass\n",
    "            return False\n",
    "\n",
    "    def set_motors(self, correction, state):\n",
    "        # ... (set_motors method)\n",
    "```\n",
    "\n",
    "## 5. Local Navigation Strategy<a name=\"local-navigation-strategy\"></a>\n",
    "\n",
    "</div style=\"width: 30%;\">\n",
    "    <img src=\"./Local_nav_model.jpeg\" />\n",
    "    <style=\"width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Figure 1: Local Navigation System Operation Diagram</p>\n",
    "</div>\n",
    "\n",
    "The Local Navigation module is designed to respond dynamically to obstacles detected by the Thymio robot's proximity sensors. The goal is to trigger Local Navigation when an obstacle is detected, ensuring the robot can navigate around it effectively. Proximity sensors' captured values trigger Local Navigation when above a desired threshold distance.\n",
    "\n",
    "#### 5.1 Assumptions<a name=\"local-assumptions\"></a>\n",
    "\n",
    "In our environment, some obstacles are strategically placed along the robot's path. The Local Navigation module operates under the following assumptions:\n",
    "\n",
    "1. **Obstacle Blocking Assumption:**\n",
    "   - The obstacles are completely blocking the robot's path. This choice aligns with our decision to perform Global Navigation only once at the program's beginning. This assumption allows Local Navigation to focus on avoiding specific obstacles without being constrained by the assumptions made during Global Navigation and thus releaving us from the necessity of recomputing the global path\n",
    "\n",
    "2. **Light Reflective Obstacle Assumption:**\n",
    "   - The obstacles are assumed to be light-reflective as the proximity sensors utilize infrared light. The sensors consist of a transmitter and a receiver, with the light reflecting off the obstacle. This assumption rules out obstacles that diffract light extensively, such as glass, ensuring reliable detection.\n",
    "\n",
    "3. **Simple Convex Geometrical Section Assumption:**\n",
    "   - Obstacles are assumed to have a simple convex geometric shape including triangles, rectangles and squares. This assumption facilitates the robot's ability to navigate around obstacles by following specific scenarios preimplemented. It ensures that the robot can circumvent obstacles without encountering complex concave shapes that might hinder navigation.\n",
    "\n",
    "#### 5.2 Operational Assumptions<a name=\"local-operational-assumptions\"></a>\n",
    "\n",
    "Given these assumptions, the Local Navigation module specifically assumes that obstacles are square-shaped. When an obstacle is detected, the entire square is considered impassable, and the robot plans its path accordingly. The assumption further specifies that the obstacle is positioned centrally within the square.\n",
    "\n",
    "This operational approach enables the robot to navigate around square obstacles effectively. However, it is important to note that the Local Navigation module does not require adjustements if different obstacle shapes are introduced due to the designed library of local paths to follow. The successful execution of Local Navigation relies on the accurate adherence to these assumptions for obstacle characteristics and the geometric layout of the environment.\n",
    "\n",
    "### 5.3 Obstacle Detection and Handling<a name=\"obstacle-detection-and-handling\"></a>\n",
    "\n",
    "\n",
    "**Obstacle Identification:** \n",
    "The robot retrieves and scales data from the proximity sensor to identify new obstacles in its path that may not have been captured by the camera. The `local_nav()` function checks if the proximity sensor in front of the robot, denoted as `x[2]`, does not detect a new obstacle. Upon the threshold of 20 being exceeded by the value of `x[2]`, the system switches to the obstacle-handling state, triggering the `obstacle_function()` when the boolean variable `obstacle`is set to true.\n",
    "\n",
    "**Obstacle Handling:**\n",
    " When an obstacle is encountered, the robot stops (`forward_robot(0)`) and assesses the position relative to the global path. Three scenarios are considered:\n",
    "  - If the global path is behind the obstacle, the robot maneuvers to reach the path (`state = obstacle_behind`).\n",
    "  - If the target is to the left of the obstacle, the robot executes a specific maneuver (`state = obstacle_left`).\n",
    "  - If the target is to the right of the obstacle, the robot follows a different maneuver (`state = obstacle_right`). \n",
    "To determine the scenario, we conduct a comparison between the robot's position in front of the obstacle (`position[0]`)]) and the subsequent position in the global path (`path[0][0]`), taking into consideration the abscissae of the positions.\n",
    "\n",
    "\n",
    "### 5.4 Maneuvering Strategies<a name=\"maneuvering-strategies\"></a>\n",
    "\n",
    "Depending on the selected senario, we will apply different maneuvering strategies. When an obstacle is encountered, the robot dynamically decides whether to turn left or right based on its initial target position relative to the obstacle. This decision influences the choice of one of the following functions. This approach enables the robot to adapt its maneuvering strategy, ensuring efficient obstacle avoidance and alignment with the next best location of the global path :\n",
    "\n",
    "- **Maneuvering Behind Obstacle (`obstacle_behind`):** \n",
    "    ```python\n",
    "        handle_obstacle_behind(path, special_cases)\n",
    "    ```\n",
    "- **Maneuvering Left of Obstacle (`obstacle_left`):**\n",
    "    ```python\n",
    "        handle_obstacle_left(position, obstacle_position)\n",
    "    ```\n",
    "- **Maneuvering Right of Obstacle (`obstacle_right`):**\n",
    "    ```python\n",
    "        handle_obstacle_right(position, obstacle_position)\n",
    "    ```\n",
    "When the target is situated behind an obstacle, two scenarios merit consideration:\n",
    "\n",
    "- If the target is behind the obstacle, and the subsequent step in the global path does not align with the predefined special cases, the robot straightforwardly moves to the position behind the obstacle.\n",
    "- If the target is behind the obstacle, and the next step in the global path corresponds to either of the two special cases previously defined (refer to the sketch above), it is more optimal for the robot to navigate directly to the second position in the global path, precisely on the special cases.\n",
    "\n",
    "Finally, once the obstacle is navigated, we enter in `state = orientation`. The robot assesses the next step in the global path to adjust its orientation with the next step in the path.To decide if it has to turn or not we just compare the abscisse of the robot and the one of the next position in the path.\n",
    "\n",
    "- **Next Step (`next_step`):** \n",
    "    ```python\n",
    "       handle_target_state(path, k)\n",
    "    ```\n",
    "\n",
    "## 6. Kalman Filter in Localization<a name=\"kalman-filter-in-localization\"></a>\n",
    "\n",
    "### 6.1 Adaptive Position Estimation<a name=\"adaptive-position-estimation\"></a>\n",
    "\n",
    "The Kalman Filter is a pivotal component in our localization strategy, adapting to changing conditions and continuously refining its estimates based on motor speed inputs. We use the kalman mainly in the case of camera obscurity or perturbation to send the estimated position and angle of the robot to the control functions in real time.\n",
    "\n",
    "\n",
    "```python\n",
    "class KalmanFilter(object):\n",
    "    def __init__(self, dim_x=4, dim_z=2, dt=0.1, dim_u=0):\n",
    "        # Initialization code\n",
    "\n",
    "    def compute_x_y_speed(self, left_motor_speed, right_motor_speed):\n",
    "        # Compute linear speed and angular speed\n",
    "        return speed, new_angle\n",
    "    \n",
    "    def predict(self, left_motor_speed, right_motor_speed, orientation, x_est_prev, P_est_prev):\n",
    "        # Measurement prediction covariance\n",
    "        S = np.dot(self.H, np.dot(self.P_est, self.H.T)) + self.R\n",
    "\n",
    "        # Kalman gain\n",
    "        K = np.dot(self.P_est, np.dot(self.H.T, np.linalg.inv(S)))\n",
    "\n",
    "        # A posteriori estimate\n",
    "        self.x_est = self.x_est + np.dot(K, i)\n",
    "        self.P_est = self.P_est - np.dot(K, np.dot(self.H, self.P_est))\n",
    "\n",
    "        # Extracting relevant information for the output...\n",
    "        return estimated_position, estimated_speed, angle, self.x_est, self.P_est\n",
    "```\n",
    "\n",
    "1. **`__init__(self, dim_x=4, dim_z=2, dt=0.1, dim_u=0):`**\n",
    "   - **Purpose:** Initializes the Kalman filter object with specified dimensions and time parameters.\n",
    "\n",
    "2. **`compute_x_y_speed(self, left_motor_speed, right_motor_speed):`**\n",
    "   - **Purpose:** Computes the linear speed (in x and y directions) and angular speed of the robot based on left and right motor speeds.\n",
    "   - **Returns:**\n",
    "     - `speed`: Linear speed of the robot.\n",
    "     - `new_angle`: Angular speed of the robot.\n",
    "\n",
    "3. **`predict(self, left_motor_speed, right_motor_speed, orientation, x_est_prev, P_est_prev):`**\n",
    "   - **Purpose:** Predicts the next state estimate using the Kalman filter based on motor speeds, orientation, and previous estimates.\n",
    "   - **Returns:**\n",
    "     - `estimated_position`: Estimated position of the robot.\n",
    "     - `estimated_speed`: Estimated linear speed of the robot.\n",
    "     - `angle`: Estimated angular speed of the robot.\n",
    "     - `self.x_est`: Updated state estimate.\n",
    "     - `self.P_est`: Updated state covariance estimate.\n",
    "\n",
    "### 6.2 Compensating for Uncertainties<a name=\"compensating-for-uncertainties\"></a>\n",
    "\n",
    "To enhance the robustness of our localization system, we explicitly consider uncertainties in motor speed measurements and orientation through the process uncertainty matrix (`Q`) and measurement uncertainty matrix (`R`). The values computed in the code are measured experimentally and tailored to fit our robot and it's purpose.\n",
    "\n",
    "### **7 Conclusion**\n",
    "\n",
    "We methodically created and included key components in this robotics project to imitate the operations of an autonomous fire rescue robot. The robot can determine its position, identify obstacles, and find goals thanks to the Vision Module's interpretation of a specified grid, feature extraction, and QR code recognition. The A* algorithm drives the Optimal Path Computation, which accurately determines the best path through a grid for the robot's precision navigation. The robot's movements are dynamically adjusted by the Motion Control Implementation, and obstructions detected by proximity sensors are skillfully responded to by the Local Navigation Strategy. In localization, the Kalman Filter improves position estimations by accounting for uncertainties. This study shows how an autonomous fire rescue robot can successfully navigate complex areas, avoid obstacles, and accomplish mission objectives.\n",
    "\n",
    "### **Main Loop**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
