{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire Truck Project Report\n",
    "\n",
    "Group members: Antoine Hinary, Francesco Maglie, Antoine Tissot-Favre, Armance Nouvel \n",
    "GitHub repo: https://github.com/fra-mgl/BOMR-project\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Vision Module](#vision-module)\n",
    "   - 2.1 [Overview](#overview)\n",
    "   - 2.2 [Environment description](#environment)\n",
    "   - 2.3 [Module structure](#vision-structure)\n",
    "   - 2.4 [Algorithms explanation](#vision-algorithms)\n",
    "      - 2.4.1 [`vision_init()` function](#vision-init)\n",
    "      - 2.4.2 [`get_thymio()` function](#get-thymio)\n",
    "    - 2.5 [References](#vision-references)\n",
    "\n",
    "3. [Optimal Path Computation](#optimal-path-computation)\n",
    "   - 3.1 [Pathfinding Precision](#pathfinding-precision)\n",
    "   - 3.2 [A* Algorithmic Mastery](#a-algorithmic-mastery)\n",
    "   - 3.3 [Navigational Options](#navigational-options)\n",
    "   - 3.4 [Path Reconstruction Elegance](#path-reconstruction-elegance)\n",
    "\n",
    "4. [Motion Control Implementation](#motion-control-implementation)\n",
    "   - 4.1 [Initialization and Configuration](#initialization-and-configuration)\n",
    "   - 4.2 [Sensor Data Retrieval and Actuation](#sensor-data-retrieval-and-actuation)\n",
    "   - 4.3 [Trajectory Following Mechanism](#trajectory-following-mechanism)\n",
    "\n",
    "5. [Local Navigation Strategy](#local-navigation-strategy)\n",
    "   - 5.1 [Assumptions](#assumptions)\n",
    "   - 5.2 [Operational Assumptions](#operational-assumptions)\n",
    "   - 5.3 [Obstacle Detection and Handling](#obstacle-detection-and-handling)\n",
    "   - 5.4 [Maneuvering Strategies](#maneuvering-strategies)\n",
    "\n",
    "6. [Kalman Filter in Localization](#kalman-filter-in-localization)\n",
    "   - 6.1 [Adaptive Position Estimation](#adaptive-position-estimation)\n",
    "   - 6.2 [Compensating for Uncertainties](#compensating-for-uncertainties)\n",
    "\n",
    "\n",
    "## **1. Introduction<a name=\"introduction\"></a>**\n",
    "\n",
    "This project aims to integrate vision, path planning, local navigation, and filtering to guide a Thymio robot on a map towards a goal. We have developed two implementations: one guided by vision and another without relying on vision. The main goal is to create a robotic system with advanced features that is comparable to an autonomous fire rescue truck. \n",
    "\n",
    "In the initial setup, a webcam captures the surroundings of the experimental area. Real-time processing using traditional image techniques extracts essential map details like the robot's position, the map itself, fixed obstacles, and the destination. The A* algorithm then calculates the best route, relaying instructions to the global controller of the Thymio robot. This controller, in turn, directs the motors to follow the optimal path. If the Thymio detects an obstacle ahead through its proximity sensors, local navigation takes charge, guiding the robot to avoid collisions. Additionally, even if the Thymio is moved unexpectedly, it can still autonomously navigate its way to the intended destination.\n",
    "\n",
    "Our project's story is based on a simulated scenario in which an autonomous fire rescue robot sets out on a mission to locate and rescue people trapped in a burning building, taking inspiration from the urgent and essential nature of firefighting scenarios. This hypothetical scenario highlights the necessity for robotic systems to manoeuvre through dangerous situations, assess the situation, and carry out precision rescue operations. It also matches the urgency and complexity of actual firefighting operations where speed and efficiency are keys for a successful salvation of the endangered victims.\n",
    "\n",
    "### **Structure**\n",
    "\n",
    "In our project's organisational structure, a central \"Command Centre\" assumes a pivotal role as the hub for coordinating the cooperation of many modules, ensuring the seamless functioning of our self-contained fire rescue robot. This crucial module takes on the tasks of establishing the Thymio connection, carrying out careful camera calibration to maximise colour and edge detection, calculating the grid map, and drawing the route for navigation. This command centre is modular in operation and handles critical robot duties such as taking quick decisions regarding the presence and absence of an obstacle. The robot operates in two different modes: local navigation, which allows for real-time obstacle avoidance, and global navigation, which plots a route to the closest checkpoint. While maintaining knowledge of the position and orientation of the robot using either the Camera module or the Kalman filter. The diagram presented below illustrates the overarching structure of our project.\n",
    "\n",
    "</div style=\"width: 30%;\">\n",
    "    <img src=\"./img/general_diagram.jpeg\" />\n",
    "    <style=\"width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Figure 2: General Diagram of the overall structure of the project</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "\n",
    "### **Modules**\n",
    "\n",
    "#### **Robot State Estimation with Extended Kalman Filter**\n",
    "\n",
    "In our autonomous ambulance rescue robot project, accurate position and speed estimation are vital for effective navigation. We utilize an Extended Kalman Filter (EKF) implemented in the `KalmanFilter` class. The state space representation is tailored to our specific needs, considering a 4-dimensional state vector $([ \\text{x}, \\text{y}, \\text{v}_x, \\text{v}_y ])$, where $(\\text{v}_x)$ and $(\\text{v}_y)$ represent the linear velocities in the x and y directions, respectively.\n",
    "\n",
    "\n",
    "##### **Coordinate System**\n",
    "\n",
    "The coordinate system follows the convention shown below.\n",
    "\n",
    "Here, $(x)$ and $(y)$ are in centimeters, and $(\\text{v}_x)$ and $(\\text{v}_y)$ are in centimeters per second.\n",
    "\n",
    "##### **Discrete Model**\n",
    "\n",
    "The discrete motion model for the Kalman filter is given by:\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\mathbf{z}_{k+1} &= f(\\mathbf{z}_k) \\\\\n",
    "\\mathbf{z}^+ &= \\begin{bmatrix} x + v_x * T_s \\\\ y + v_y * T_s \\\\ v_x \\\\ v_y \\end{bmatrix} \n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "where $(\\text{v}_x)$ and $(\\text{v}_y)$ are computed as\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{v}_x = \\dfrac{v_r + v_l}{2} * \\cos \\theta * \\alpha \\\\\n",
    "\\text{v}_y = \\dfrac{v_r + v_l}{2} * \\sin \\theta * \\alpha\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $v_r$ and $v_l$ are the motor speeds (in RPM) and $\\alpha$ is the speed conversion factor.<br>\n",
    "Then, to compute the angle the robot has in the followin instant in time (ie. $\\hat{\\theta}$), we performe\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\omega = \\dfrac{v_r - v_l}{d} * \\alpha \\\\\n",
    "\\Delta \\theta = \\omega * T_s \\\\\n",
    "\\hat{\\theta} = \\theta + \\Delta \\theta\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "##### **Measurement Model**\n",
    "\n",
    "The discrete measurement model is given by:\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\mathbf{z}_{k+1} &= h(\\mathbf{z}_k) \\\\\n",
    "\\mathbf{z}^+ &= \\begin{bmatrix} x_{\\text{{measured}}} \\\\ y_{\\text{{measured}}} \\\\ v_{x \\text{{ measured}}} \\\\ v_{y \\text{{ measured}}} \\end{bmatrix} \n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "The measurement model incorporates data from the camera and the motor encoders, providing measured values for position and motor speeds. The covariance matrix $\\mathbf{R}$ captures the measurement noise.\n",
    "\n",
    "##### **Extended Kalman Filter Algorithm**\n",
    "\n",
    "The steps of the extended Kalman filter algorithm, tailored to our implementation, are as follows:\n",
    "\n",
    "1. **Prediction Step:**\n",
    "In this step, the Kalman filter predicts the next state of the system based on the previous state estimate and the system dynamics, incorporating the process model. The predicted state and covariance represent the system's expected behavior, considering motion and process uncertainties :\n",
    "\n",
    "   \\begin{align*}\n",
    "   \\hat{\\mathbf{x}}_{k \\mid k-1} &= f(\\hat{\\mathbf{x}}_{k-1 \\mid k-1}, \\mathbf{u}_k) \\\\\n",
    "   \\mathbf{P}_{k \\mid k-1} &= \\mathbf{F}_k \\mathbf{P}_{k-1 \\mid k-1} \\mathbf{F}_k^T + \\mathbf{Q}_k\n",
    "   \\end{align*}\n",
    "\n",
    "2. **Measurement Residual:**\n",
    "The measurement residual calculates the difference between the predicted measurement (obtained from the predicted state) and the actual measurement. It represents the discrepancy or error between the expected and observed values, capturing the system's deviation from the predicted state : \n",
    "\n",
    "   \\begin{align*}\n",
    "   \\tilde{\\mathbf{y}}_k &= \\mathbf{z}_k - h(\\hat{\\mathbf{x}}_{k \\mid k-1})\n",
    "   \\end{align*}\n",
    "\n",
    "3. **Measurement Update:**\n",
    "In the measurement update step, the Kalman filter refines its state estimate by incorporating the measured values. The Kalman gain is calculated to give more weight to reliable measurements and less weight to uncertain ones. The updated state estimate and covariance reflect a more accurate representation of the system's true state, considering both prediction and measurement inform : \n",
    "\n",
    "   \\begin{align*}\n",
    "   \\mathbf{S}_k &= \\mathbf{H}_k \\mathbf{P}_{k \\mid k-1} \\mathbf{H}_k^T + \\mathbf{R}_k \\\\\n",
    "   \\mathbf{K}_k &= \\mathbf{P}_{k \\mid k-1} \\mathbf{H}_k^T (\\mathbf{S}_k)^{-1} \\\\\n",
    "   \\hat{\\mathbf{x}}_{k \\mid k} &= \\hat{\\mathbf{x}}_{k \\mid k-1} + \\mathbf{K}_k \\tilde{\\mathbf{y}}_k \\\\\n",
    "   \\mathbf{P}_{k \\mid k} &= (\\mathbf{I} - \\mathbf{K}_k \\mathbf{H}_k) \\mathbf{P}_{k \\mid k-1}\n",
    "   \\end{align*}\n",
    "\n",
    "\n",
    "##### **Estimation of Ts (Time difference)**\n",
    "\n",
    "The accurate estimation of the time between iterations (\\(T_s\\)) is essential for the dynamic performance of the Kalman filter, directly influencing temporal alignment, motion prediction, and covariance matrix updates. A precise \\(T_s\\) ensures that predictions align with the system's temporal dynamics, maintaining the filter's adaptability to dynamic changes. Inaccuracies in \\(T_s\\) can disrupt temporal alignment, affecting the filter's ability to provide accurate state estimates and impacting control actions.\n",
    "\n",
    "##### **Estimation of Noise Matrices**\n",
    "\n",
    "For the motion model:<br />\n",
    "$$\n",
    "Q=\\begin{bmatrix}\n",
    "0.04 & 0 & 0 & 0\\\\\n",
    "0 & 0.04 & 0 & 0\\\\\n",
    "0 & 0 & 6 & 0\\\\\n",
    "0 & 0 & 0 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We derived the values of 0.04 and the angle through empirical estimation, without a specific deterministic basis. Regarding the speed, empirical data obtained during the eighth week informed our estimation process, wherein half of the variance was attributed to the inherent motion dynamics, while the remaining half originated from measurement uncertainties. <br>\n",
    "\n",
    "For the measurement model:<br />\n",
    "$$\n",
    "R=\\begin{bmatrix}\n",
    "0.25 & 0 & 0 & 0\\\\\n",
    "0 & 0.25 & 0 & 0\\\\\n",
    "0 & 0 &6 & 0\\\\\n",
    "0 & 0 & 0 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These values are estimated, with half of the variance attributed to motion and the other half to measurement.\n",
    "\n",
    "\n",
    "## 2. Vision Module<a name=\"vision-module\"></a>\n",
    "\n",
    "### 2.1 Overview<a name=\"overview\"></a>\n",
    "\n",
    "The vision module serves as the visual intelligence of the Thymio robot, forming a critical component within the broader project framework. Its primary purpose revolves around interpreting the surrounding environment recognizing specific visual elements. The objectives of this module include determining the robot's position and all the order elements in the environment.\n",
    "\n",
    "The OpenCV library is used to performe these visual algorithms.<br>\n",
    "To be able to use the ArUco library, you should install a specific version of OpenCV. To do so, you can run these commands:<br>\n",
    "\n",
    "        pip uninstall opencv-python\n",
    "        pip install opencv-contrib-python==4.6.0.66\n",
    "\n",
    "\n",
    "### 2.2 Environment description<a name=\"environment\"></a>\n",
    "\n",
    "The environment is defined by a white grid of arbitrary dimensions and each cell has the same dimention as the Thymio robot. To identify the grid, we use 4 ArUco markers placed on the corners of it.\n",
    "We then define 3 types of objects that can be detected in the environment; these are squares and each should be not larger than a cell of the grid. The objects categories are<br>\n",
    "- the static obstacles, which are in red\n",
    "- the middle target, which is in blue\n",
    "- the final target, which is in green\n",
    "\n",
    "Lastly, to estimate the pose of the robot, we use another ArUco marker.<br>\n",
    "\n",
    "We made the choice of using the ArUco markers because they are a very reliable way to extract the position of an object; moreover it's easy to find documentation on how to use them since they are widely use in a lot of computer vision applications.<br>\n",
    "Then, the simple shape of the objects and the pronounced difference in the colors let the user easily identify them without the need of using other markers. \n",
    "\n",
    "</div style=\"width: 30%;\">\n",
    "    <img src=\"./img/env_map.jpg\" />\n",
    "    <style=\"width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Map setup </p>\n",
    "</div>\n",
    "\n",
    "### 2.3 Module structure<a name=\"vision-structure\"></a>\n",
    "\n",
    "All the code responsible for the visual feature extraction is placed in the module `vision`. The main components of this module are two classes, used to describe the visual elements, and two functions, which are called in the main loop of the code to performe the visual tasks.\n",
    "\n",
    "The two classes that have been defined are (in file VisionObjects.py):<br>\n",
    "- `VisionObject`: to handle a generic element in the environment. Then we define 3 subclasses, one for each caterogy of objects in the environment\n",
    "- `Thymio`: a specific class to handle all the tasks directly associated with the Thymio\n",
    "\n",
    "Then, in the main loop, we need to call two functions from the Vision module:\n",
    "- `vision_init()`: to extract the grid and all the visual elements \n",
    "- `get_thymio()` : to extract the Thymio's pose\n",
    "\n",
    "### 2.4 Algorithm explanation<a name=\"vision-algorithms\"></a>\n",
    "\n",
    "#### 2.4.1 `vision_init()` function<a name=\"vision-init\"></a>\n",
    "\n",
    "Let's begin by delving into the `vision_init()` function to understand what are the main steps that are performed (in parenthesis, one can find a reference to the file that contains such function).\n",
    "\n",
    "1. Grid extraction (vision.detection.grid_extraction)<br>\n",
    "    As said before, we used 4 ArUco markers to identify the corners and extract the grid. To identify such markers, we use the built-in functions in the OpenCV library.<br>\n",
    "    Once defined the type of ArUco we are using, we just need to use the following snipped of code to detect them:<br>\n",
    "    \n",
    "        arucoDict = cv.aruco.Dictionary_get(cv.aruco.DICT_5X5_1000)\n",
    "        arucoParams = cv.aruco.DetectorParameters_create()\n",
    "        positions, ids, rejected = cv.aruco.detectMarkers(image, arucoDict, parameters=arucoParams)\n",
    "    \n",
    "\n",
    "    Since each marker has a specific ID, we can exploit this information to know which marker is in which corner and then extracting the grid.\n",
    "\n",
    "2. Perspective transformation (vision.detection.perspective_transform)<br>\n",
    "\tWe then need to correct the perspective of the image. To do so, we use the built-in function that compute a linear transformation from the original image to the corrected one (`matrix M` which is 3x3). Then we apply it to each pixel as following\n",
    "\n",
    "    $$(\\hat{x}, \\hat{y}, \\alpha)^T = M * (x, y, 1)^T$$\n",
    "    where $(x, y)$ are the coordinates of the original pixel. To get the coordinates of the transformed one, $\\hat{x}$ and $\\hat{y}$ are divided by $\\alpha$, yielding\n",
    "    \n",
    "    $$ (\\dfrac{\\hat{x}}{\\alpha}, \\dfrac{\\hat{y}}{\\alpha}) $$\n",
    "\n",
    "    The snipped of code to achieve this is the following:\n",
    "\n",
    "        M = cv.getPerspectiveTransform(src_points, dst_points)\n",
    "        dst = cv.warpPerspective(source, M, dst_dim)\n",
    "\n",
    "    where `dst` is the corrected image.\n",
    "        \n",
    "\n",
    "3. Recognition (vision.functions.recognition)<br>\n",
    "To recognise the objects (given the fact they have different colours), we apply this algorithm for each class of objects (ie. for each color):<br>\n",
    "\n",
    "- extract a mask for the given color (vision.detection.colored_object_extraction)<br>\n",
    "                a. change colorspace from RGB to HSV (in this space it’s easier to detect colours)<br>\n",
    "                b. compute the mask for the color we are looking for<br>\n",
    "                c. performe a morphology opening (erosion followed by dilation): the objective of this step is to remove small elements (usually generated by noise)<br>\n",
    "                d. find contours with OpenCV built-in function; it returns a list of points belonging to the contour of the object <br>\n",
    "\n",
    "                contours, _ = cv.findContours(thresh, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_NONE)\n",
    "\n",
    "- given the contours, we create one instance for each object and we compute the coordinates of each one in the grid.<br>\n",
    "    The class `VisionObject` defines two methods to achieve this (vision.VisionObjects)<br>\n",
    "        - `find_center()`: the mean on x and y of each point found by the `findContours()` function<br>\n",
    "        - `compute_grid_coordinates()`: knowing the dimension of the grid (in pixel) and the number of cells, we compute the position in the grid<br>\n",
    "\n",
    "                y = floor(self.center_pixel[0] / grid_width_pixel * grid_width_cells)\n",
    "                x = floor(self.center_pixel[1] / grid_height_pixel * grid_height_cells)\n",
    "                    \n",
    "- compute the output of the function<br>\n",
    "        Knowing the position of each obstacle, we can create an occupancy grid, ie. a grid in which we set 1 if there is an obstacle (occupied cell), 0 otherwise.<br>\n",
    "        For the middle and final targets, we define them as lists.\n",
    "\n",
    "\n",
    "#### 2.4.2 `get_thymio()` function<a name=\"get-thymio\"></a>\n",
    "Let's now take a look to the `get_thymio()` function, which is used to retrive the pose of the robot, ie. x and y coordinates and the orientation angle.\n",
    "\n",
    "1. Grid extraction and perspective correction (as before)\n",
    "\n",
    "2. `thymio_recognition()` (vision.function.thymio_recognition)\n",
    "\t- find the ArUco marker as before, but this time we just need to detect the Thymio's marker, which is defined by `id = 5`\n",
    "\t- performe `thymio.pose_estimation()`<br>\n",
    "\n",
    "                thymio.pose_estimation(env.shape[1], env.shape[0])\n",
    "        Now we need to find the coordinates and the orientation of the robot. To compute the coordinates, we performe the same as before to compute the coordinates of each object in the environment<br>\n",
    "\n",
    "\t\t        self.find_center()\n",
    "                self.compute_grid_coordinates(grid_width_pixel, grid_height_pixel)\n",
    "\t    To compute the orientation angle, we use the function `compute_theta()`\n",
    "    \n",
    "                theta = self.compute_theta()\n",
    "        Using the ArUco library, we can detect the four corners of the marker. Knowing them, we compute the two line connecting the corners’ pairs 0-3 and 1-2 and the corresponding angle. To do that, we exploit the relation between the angle and the angular coefficient of the line:\n",
    "\n",
    "        $$ \\theta = arctan2(\\Delta y, \\Delta x)$$\n",
    "\n",
    "        The use of the function $arctan2$ ensures us to have $\\theta \\in (- \\pi, + \\pi]$\n",
    "                \n",
    "        We compute both angles and then we take the average in order to be more robust to noise and be more precise.<br>\n",
    "        The last thing we need to do is a change of reference: since we are using pixels to compute the lines and thus the angle, the image frame and the one used by the Thymio to describe the environment are not the same. The transformation is the following:\n",
    "        $$ \\hat{\\theta} = - \\theta + 90 $$\n",
    "        and it is performed inside the `fix_theta()` function.<br>\n",
    "        A figure is provided below to visualize the difference between the two references.\n",
    "\n",
    "</div style=\"width: 30%;\">\n",
    "    <img src=\"./img/angle_reference.jpg\" />\n",
    "    <style=\"width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Reference frames </p>\n",
    "</div>\n",
    "\n",
    "    \n",
    "### 2.5 References<a name=\"vision-references\"></a>\n",
    "OpenCV: https://opencv.org<br>\n",
    "ArUco markers:<br>\n",
    "    https://pyimagesearch.com/2020/12/21/detecting-ArUco-markers-with-opencv-and-python/<br>\n",
    "    https://github.com/niconielsen32/ComputerVision/tree/master/ArUco<br>\n",
    "Perspective transform: https://medium.com/analytics-vidhya/opencv-perspective-transformation-9edffefb2143<br>\n",
    "Morphology opening: https://docs.opencv.org/3.4/d9/d61/tutorial_py_morphological_ops.html<br>\n",
    "Color extraction: https://docs.opencv.org/3.4/df/d9d/tutorial_py_colorspaces.html<br>\n",
    "Find contours: https://docs.opencv.org/3.4/d4/d73/tutorial_py_contours_begin.html\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 3. Optimal Path Computation<a name=\"optimal-path-computation\"></a>\n",
    "The A algorithm was implemented using the exercise set 5 of the course, it was adapted to fit our environment (a rectangular grid) by adding `max_val_x` and `max_val_y` values. \n",
    "### 3.1 Pathfinding Precision<a name=\"pathfinding-precision\"></a>\n",
    "\n",
    "This section delves into global navigation, unveiling the Thymio's optimal path computation facilitated by the A* algorithm. Beyond simple movement, this algorithmic approach strives to map the most efficient course for the robot through a predefined grid, minimizing the cost to achieve the desired result.\n",
    "\n",
    "### 3.2 A* Algorithmic Mastery<a name=\"a-algorithmic-mastery\"></a>\n",
    "\n",
    "The A* algorithm, pivotal in our pathfinding strategy, involves key steps:\n",
    "Initialization establishes the start and goal.\n",
    "A heuristic guides efficient exploration.\n",
    "Management of open and closed sets optimizes node evaluation.\n",
    "Cost functions track path costs, aiding optimal route decisions.\n",
    "Defined movement options dictate allowable transitions and costs.\n",
    "The main loop iteratively evaluates nodes until reaching the goal or depleting the open set.\n",
    "Node expansion assesses neighbors, updating paths if more cost-effective options are found.\n",
    "Path reconstruction constructs the optimal path upon goal attainment.\n",
    "\n",
    "### 3.3 Navigational Options<a name=\"navigational-options\"></a>\n",
    "\n",
    "Two sets of movement options, 4-connectivity and 8-connectivity, are available within the A* algorithm. Opting for 4N connectivity aligns with practical constraints, ensuring computational efficiency in navigating the constrained layout of the simulated environment, mirroring realistic scenarios with restricted diagonal movements.\n",
    "\n",
    "### 3.4 Path Reconstruction Elegance<a name=\"path-reconstruction-elegance\"></a>\n",
    "\n",
    "Efficient path reconstruction in the A* algorithm's code ensures the Thymio navigates carefully and reaches its destination. Utilizing the reconstruct_path function, the algorithm traces back each step, creating a list of visited nodes for the optimal path. This list, containing nodes in order from start to goal, serves as a smooth roadmap for the Thymio's precise navigation.\n",
    "\n",
    "## 4. Motion Control Implementation Overview<a name=\"motion-control-implementation-overview\"></a>\n",
    "\n",
    "### 4.1 Initialization and Configuration<a name=\"initialization-and-configuration\"></a>\n",
    "\n",
    "The motion control process initiates with the instantiation of the `Control` class, where parameters for motion control, including the proportional (kp), integral (ki), and derivative (kd) gains, as well as the robot's wheel base width, are defined. These parameters establish the foundation for subsequent motion control strategies. Additionally, the class contains methods for sensor data retrieval and actuation, contributing to the overall adaptability of the robot's motion.\n",
    "\n",
    "### 4.2 Sensor Data Retrieval and Actuation<a name=\"sensor-data-retrieval-and-actuation\"></a>\n",
    "\n",
    "The `Control` class features a method named `get_sensors` responsible for retrieving sensor data such as motor speeds and horizontal proximity values. This method provides essential information for making informed decisions during the motion control process and also for the local avoidance tasks. The `set_motors_PID` method is implemented to adjust the left and right motor speeds based on PID control, enhancing the adaptability and responsiveness of the robot to environmental changes and thus limiting the angular error allowed for the robot.\n",
    "\n",
    "### 4.3 PID Controller Implementation<a name=\"pid-controller-implementation\"></a>\n",
    "\n",
    "A PID controller is implemented within the `pid_controller` function, utilizing proportional, integral, and derivative terms to dynamically adjust motor speeds. The control gains (kp, ki, kd) and the robot's width play a crucial role in determining the correction applied to the motors. The function also includes a mechanism to handle aggressive corrections for significant angle deviations, ensuring effective turning when needed. The `normalize_angle` function is utilized to maintain the angle within the range of -180 to 180 degrees.\n",
    "\n",
    "**Table of Constants for Motion Control:**\n",
    "\n",
    "| Constant        | Value/Description                          |\n",
    "|-----------------|--------------------------------------------|\n",
    "| kp              | 2.0                                        |\n",
    "| ki              | 0.04                                       |\n",
    "| kd              | 0.01                                       |\n",
    "| robot_width     | 0.1                                        |\n",
    "\n",
    "This set of constants is meticulously chosen to balance the responsiveness and stability of the robot's motion control, ensuring effective navigation in various scenarios. The proportional, integral, and derivative gains are fine-tuned to achieve optimal performance, while the robot's width is considered for accurate motion calculations.\n",
    "\n",
    "\n",
    "## 5. Local Navigation Strategy<a name=\"local-navigation-strategy\"></a>\n",
    "\n",
    "Reference : For this module, we utilize the lab of the course about the local navigation.\n",
    "\n",
    "The Local Navigation module is designed to respond dynamically to obstacles detected by the Thymio robot's proximity sensors. The goal is to trigger Local Navigation when an obstacle is detected, ensuring the robot can navigate around it effectively. Proximity sensors' captured values trigger Local Navigation when above a desired threshold distance.\n",
    " \n",
    " - Key parameters :\n",
    "\n",
    "| Variables              | Meaning                                                      | Type                 |\n",
    "|------------------------|--------------------------------------------------------------|----------------------|\n",
    "| `sens`                 | The initial direction of movement ('Right' or 'Left')        | string               |\n",
    "| `obstacle`             | Boolean indicating whether an obstacle is detected           | bool                 |\n",
    "| `special_step`         | The determining special case from the list of `special_cases`| int                  |\n",
    "| `status`               | Index of the step in the path after the obstacle handling    | int                  |\n",
    "| `orientation`          | Orientation of the Thymio                                    | int                  |\n",
    "| `special_cases`        | List of 2 cases that need specific handling during obstacles | int array (1*2)      |\n",
    "| `k`                    | Index to determine which point of the path we are taking     | int array ()         |\n",
    "| `path`                 | Global path of the Thymio                                    | (type not specified) |\n",
    "| `position`             | Position of the Thymio in front of the obstacle              | int array (1*2)      |\n",
    "| `prox[]`               | List of all values of the proximity sensors                  | float array          |\n",
    "\n",
    " - Key functions : \n",
    "\n",
    "| Function                   | Input                                          | Output                                       |\n",
    "|----------------------------|------------------------------------------------|----------------------------------------------|\n",
    "| `local_nav`                | (An instance of the Thymio robot class)      | boolean indicating whether an obstacle is detected              |\n",
    "| `obstacle_extraction`      | (grid of obstacles)                            | list of obstacle's coordinates               |\n",
    "| `obstacle_function`        | (path, position, angle, thymio, obstacle_coordinates)| index of the next step in path              |\n",
    "| `handle_final_orientation` | (path, k, axe, thymio)                         | updated state after handling the final orientation                     |\n",
    "| `rotate_robot`             | (angle_degrees, rotation_speed, sens, thymio) | (no output)                     |\n",
    "| `forward_robot`            | (motor_speed, distance, thymio)                | (no output)                      |\n",
    "| `determine_special_path`   | (path, special_cases, axe, special_step)      | The updated value of `special_step`                    |\n",
    "\n",
    "\n",
    "The diagram showcases the architecture of our \"Local Navigation\" module, a critical component enabling real-time obstacle avoidance and autonomous navigation. Through interconnected sub-modules, it orchestrates obstacle detection, path planning, and motor control, ensuring the robot navigates dynamically and adapts to its environment.\n",
    "\n",
    "</div style=\"width: 30%;\">\n",
    "    <img src=\"./model_local_nav.jpeg\" />\n",
    "    <style=\"width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Figure 3: Local Navigation System Operation Diagram</p>\n",
    "</div>\n",
    "\n",
    "### 5.1 Assumptions<a name=\"local-assumptions\"></a>\n",
    "\n",
    "In our environment, some obstacles are strategically placed along the robot's path. The Local Navigation module operates under the following assumptions:\n",
    "\n",
    "1. **Obstacle Blocking Assumption:**\n",
    "   - The obstacles are completely blocking the robot's path. This choice aligns with our decision to perform Global Navigation only once at the program's beginning. This assumption allows Local Navigation to focus on avoiding specific obstacles without being constrained by the assumptions made during Global Navigation and thus releaving us from the necessity of recomputing the global path\n",
    "\n",
    "2. **Light Reflective Obstacle Assumption:**\n",
    "   - The obstacles are assumed to be light-reflective as the proximity sensors utilize infrared light. The sensors consist of a transmitter and a receiver, with the light reflecting off the obstacle. This assumption rules out obstacles that diffract light extensively, such as glass, ensuring reliable detection.\n",
    "\n",
    "3. **Simple Convex Geometrical Section Assumption:**\n",
    "   - Obstacles are assumed to have a simple convex geometric shape including triangles, rectangles and squares. This assumption facilitates the robot's ability to navigate around obstacles by following specific scenarios preimplemented. It ensures that the robot can circumvent obstacles without encountering complex concave shapes that might hinder navigation.\n",
    "\n",
    "### 5.2 Operational Assumptions<a name=\"local-operational-assumptions\"></a>\n",
    "\n",
    "Given these assumptions, the Local Navigation module specifically assumes that obstacles are square-shaped. When an obstacle is detected, the entire square is considered impassable, and the robot plans its path accordingly. The assumption further specifies that the obstacle is positioned centrally within the square.\n",
    "\n",
    "This operational approach enables the robot to navigate around square obstacles effectively. However, it is important to note that the Local Navigation module does not require adjustements if different obstacle shapes are introduced due to the designed library of local paths to follow. The successful execution of Local Navigation relies on the accurate adherence to these assumptions for obstacle characteristics and the geometric layout of the environment.\n",
    "\n",
    "### 5.3 Obstacle Detection and Handling<a name=\"obstacle-detection-and-handling\"></a>\n",
    "\n",
    "\n",
    "**Obstacle Identification:** \n",
    "The robot retrieves and scales data from the proximity sensor to identify new obstacles in its path that may not have been captured by the camera. The `local_nav()` function checks if the proximity sensor in front of the robot, denoted as `x[2]`, does not detect a new obstacle. Upon the threshold of 20 being exceeded by the value of `x[2]`, the system switches to the obstacle-handling state, triggering the `obstacle_function()` when the boolean variable `obstacle`is set to true.\n",
    "\n",
    "**Obstacle Handling:**\n",
    " When an obstacle is encountered, the robot stops and assesses the position relative to the global path. Three scenarios are considered:\n",
    "  - If the global path is behind the obstacle, the robot maneuvers to reach the path (`state = obstacle_behind`).\n",
    "  - If the target is to the left of the obstacle, the robot executes a specific maneuver (`state = obstacle_left`).\n",
    "  - If the target is to the right of the obstacle, the robot follows a different maneuver (`state = obstacle_right`). \n",
    "To determine the scenario, we conduct a comparison between the robot's position in front of the obstacle (`position[axe]`) and the subsequent position in the global path (`path[axe][1]`), taking into consideration the abscissae of the positions. The comparison coordinate is determined by the Thymio's orientation in front of the obstacle:\n",
    "  - If the orientation is 180 or 0 degrees, `axe` is set to 1.\n",
    "  - If the orientation is 90 or -90 degrees, `axe` is set to 0.\n",
    "\n",
    "\n",
    "### 5.4 Maneuvering Strategies<a name=\"maneuvering-strategies\"></a>\n",
    "\n",
    "Depending on the selected senario, we will apply different maneuvering strategies. When an obstacle is encountered, the robot dynamically decides whether to turn left or right based on its initial target position relative to the obstacle. This decision influences the choice of one of the following functions. This approach enables the robot to adapt its maneuvering strategy, ensuring efficient obstacle avoidance and alignment with the next best location of the global path :\n",
    "\n",
    "- **Maneuvering Behind Obstacle (`obstacle_behind`):** \n",
    "    `handle_target_behind(special_step, \"Right\", k, thymio,obstacle_coordinates,position,orientation, axe)`\n",
    "- **Maneuvering Left of Obstacle (`obstacle_left`):**\n",
    "    `handle_target_left(thymio)`\n",
    "- **Maneuvering Right of Obstacle (`obstacle_right`):**\n",
    "    `handle_target_right(thymio)`\n",
    "When the target is situated behind an obstacle, two scenarios merit consideration:\n",
    "\n",
    "- If the target is behind the obstacle (case 2 on the sketch above), and the subsequent step in the global path does not align with the predefined special cases, the robot straightforwardly moves to the position behind the obstacle. The thymio dynamically adjusts its trajectory based on current obstacles. The `obstacle_extraction(obs_grid)` function extracts a list of obstacle coordinates, which is then compared with the robot's position to determine obstacles on its left or right. If detected, the `handle_target_behind` function adapts the path accordingly.\n",
    "\n",
    "- If the target is behind the obstacle, and the next step in the global path corresponds to either of the two special cases previously defined (cases 1 and 3 on the sketch above), it is more optimal for the robot to navigate directly to the second position in the global path, precisely on the special cases.\n",
    "\n",
    "Finally, once the obstacle is navigated, we enter in `state = orientation`. The robot assesses the next step in the global path to adjust its orientation with the next step in the path. To decide if it has to turn or not we just compare the abscisse of the robot and the one of the next position in the path.\n",
    "\n",
    "- **Next Step (`next_step`):** \n",
    "    `handle_final_orientation(path, k,axe, thymio)`\n",
    "   \n",
    "\n",
    "## 6. Kalman Filter in Localization<a name=\"kalman-filter-in-localization\"></a>\n",
    "\n",
    "### 6.1 Adaptive Position Estimation<a name=\"adaptive-position-estimation\"></a>\n",
    "\n",
    "The Kalman Filter is a pivotal component in our localization strategy, adapting to changing conditions and continuously refining its estimates based on motor speed inputs. We use the kalman mainly in the case of camera obscurity or perturbation to send the estimated position and angle of the robot to the control functions in real time. The Kalman is structures as follows : first the functoin called **compute_x_y_speed(self, left_motor_speed, right_motor_speed)** is called taking as inputs the left and right motor speeds, it then computes the speeds in term of linear speed (v_r - v_l)/2 , the angular speed, the the vectorial speeds [v_x and v_y] and finally uses the difference between the 2 wheels' speed to compute the angular variation $\\Delta\\theta$ to estimate the actual orienation and angle of the our Robot.\n",
    "\n",
    "In our state space representation, we chose not to include the angle (\\(\\theta\\)) as a primary state variable to enhance computational efficiency. Instead, we compute and estimate \\(\\theta\\) separately using the motor inputs from sensor values. This modular approach streamlines computations, reduces complexity, and aligns with the specific requirements of our application. In addition, the speeds returned from the functions are then being passed through our extended Kalman Filter to ultimately conclude our estimation for the position which is the most important element for which we have the estimator. In addition to our global choices, we have decided to facilitate the readability of the filtering task and merge both steps explained above for this task : the **prediction** and the **update** in a single predict function that outputs the expected status of the system and the updated matrix **P** as well.\n",
    "\n",
    "#### Constants used for the filtering \n",
    "\n",
    "| Constant                   | Value or Description            | Justification                                                                |\n",
    "|-----------------------------|--------------------------------|------------------------------------------------------------------------------|\n",
    "| `dt`                        | 0.08                                          | Time step for the state transition matrix by default untill the comuted time is passed to the function                    |\n",
    "| `speed_conv_factor`         | 0.35                                           |Conversion factor for motor speed to linear speed computed by observing the robot for a distance of 12cm with a speed ot 80 and computing the \\(\\delta t\\) thus extracting the factor             |\n",
    "| `wheel_width`               | 100 (distance between the two wheels in mm)    | Distance between the two wheels of the robot used to compute the angular speed of the robot             |\n",
    "\n",
    "\n",
    "```python\n",
    "class KalmanFilter(object):\n",
    "    def __init__(self, dim_x=4, dim_z=2, dt=0.1):\n",
    "        # Initialization code\n",
    "\n",
    "    def compute_x_y_speed(self, left_motor_speed, right_motor_speed):\n",
    "        # Compute linear speed and angular speed\n",
    "        return speed, new_angle\n",
    "    \n",
    "    def predict(self, left_motor_speed, right_motor_speed, orientation, x_est_prev, P_est_prev):\n",
    "        # Measurement prediction covariance\n",
    "        S = np.dot(self.H, np.dot(self.P_est, self.H.T)) + self.R\n",
    "\n",
    "        # Kalman gain\n",
    "        K = np.dot(self.P_est, np.dot(self.H.T, np.linalg.inv(S)))\n",
    "\n",
    "        # A posteriori estimate\n",
    "        self.x_est = self.x_est + np.dot(K, i)\n",
    "        self.P_est = self.P_est - np.dot(K, np.dot(self.H, self.P_est))\n",
    "\n",
    "        # Extracting relevant information for the output...\n",
    "        return estimated_position, estimated_speed, angle, self.x_est, self.P_est\n",
    "```\n",
    "\n",
    "1. **`__init__(self, dim_x=4, dim_z=2, dt=0.1):`**\n",
    "   - **Purpose:** Initializes the Kalman filter object with specified dimensions and time parameters.\n",
    "\n",
    "2. **`compute_x_y_speed(self, left_motor_speed, right_motor_speed):`**\n",
    "   - **Purpose:** Computes the linear speed (in x and y directions) and angular speed of the robot based on left and right motor speeds.\n",
    "   - **Returns:**\n",
    "     - `speed`: Linear speed of the robot.\n",
    "     - `new_angle`: The robot's new estimated angle.\n",
    "\n",
    "3. **`predict(self, left_motor_speed, right_motor_speed, orientation, x_est_prev, P_est_prev):`**\n",
    "   - **Purpose:** Predicts the next state estimate using the Kalman filter based on motor speeds, orientation, and previous estimates.\n",
    "   - **Returns:**\n",
    "     - `estimated_position`: Estimated position of the robot.\n",
    "     - `estimated_speed`: Estimated linear speed of the robot.\n",
    "     - `angle`: Estimated angular speed of the robot.\n",
    "     - `self.x_est`: Updated state estimate.\n",
    "     - `self.P_est`: Updated state covariance estimate.\n",
    "\n",
    "### 6.2 Compensating for Uncertainties<a name=\"compensating-for-uncertainties\"></a>\n",
    "\n",
    "To enhance the robustness of our localization system, we explicitly consider uncertainties in motor speed measurements and orientation through the process uncertainty matrix (`Q`) and measurement uncertainty matrix (`R`). The values computed in the code are either measured experimentally or extracted from the Practical session number 8 of the course of Mobie Robotics with Professor Francesco Mondada.\n",
    "\n",
    "\n",
    "## **7 Conclusion**\n",
    "\n",
    "We methodically created and included key components in this robotics project to imitate the operations of an autonomous fire rescue robot. The robot can determine its position, identify obstacles, and find goals thanks to the Vision Module's interpretation of a specified environment. The A* algorithm drives the Optimal Path Computation, which accurately determines the best path through a grid for the robot's precision navigation. The robot's movements are dynamically adjusted by the Motion Control Implementation, and obstructions detected by proximity sensors are skillfully responded to by the Local Navigation Strategy. In localization, the Kalman Filter improves position estimations by accounting for uncertainties. This study shows how an autonomous fire rescue robot can successfully navigate complex areas, avoid obstacles, and accomplish mission objectives.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
