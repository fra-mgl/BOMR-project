{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire Truck Project Report\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "1. [Introduction](#introduction)\n",
    "2. [Vision Module](#vision-module)\n",
    "   - 2.1 [Overview](#overview)\n",
    "   - 2.2 [Reading the Predefined Grid](#reading-the-predefined-grid)\n",
    "   - 2.3 [Feature Extraction](#feature-extraction)\n",
    "   - 2.4 [QR Code Identification](#qr-code-identification)\n",
    "   - 2.5 [Objectives](#objectives)\n",
    "      - 2.5.1 [Determining the Robot's Position](#determining-the-robots-position)\n",
    "      - 2.5.2 [Recognizing Obstacles (In Red)](#recognizing-obstacles)\n",
    "      - 2.5.3 [Identifying the Goal (In Blue)](#identifying-the-goal)\n",
    "   - 2.6 [Pre-processing for Vision](#pre-processing)\n",
    "   - 2.7 [Mapping and Localization](#mapping-and-localization)\n",
    "\n",
    "3. [Optimal Path Computation](#optimal-path-computation)\n",
    "   - 3.1 [Pathfinding Precision](#pathfinding-precision)\n",
    "   - 3.2 [A* Algorithmic Mastery](#a-algorithmic-mastery)\n",
    "   - 3.3 [Navgational Options](#navigational-options)\n",
    "   - 3.4 [Path Reconstruction Elegance](#path-reconstruction-elegance)\n",
    "\n",
    "4. [Motion Control Implementation](#motion-control-implementation)\n",
    "   - 4.1 [Initialization and Configuration](#initialization-and-configuration)\n",
    "   - 4.2 [Sensor Data Retrieval and Actuation](#sensor-data-retrieval-and-actuation)\n",
    "   - 4.3 [Trajectory Following Mechanism](#trajectory-following-mechanism)\n",
    "\n",
    "5. [Local Navigation Strategy](#local-navigation-strategy)\n",
    "   - 5.1 [Assumptions](#assumptions)\n",
    "   - 5.2 [Operational Assumptions](#operational-assumptions)\n",
    "   - 5.3 [Obstacle Detection and Handling](#obstacle-detection-and-handling)\n",
    "   - 5.4 [Maneuvering Strategies](#maneuvering-strategies)\n",
    "\n",
    "6. [Kalman Filter in Localization](#kalman-filter-in-localization)\n",
    "   - 6.1 [Adaptive Position Estimation](#adaptive-position-estimation)\n",
    "   - 6.2 [Compensating for Uncertainties](#compensating-for-uncertainties)\n",
    "\n",
    "\n",
    "## **1. Introduction<a name=\"introduction\"></a>**\n",
    "\n",
    "This project aims to integrate vision, path planning, local navigation, and filtering to guide a Thymio robot on a map towards a goal. We have developed two implementations: one guided by vision and another without relying on vision. The main goal is to create a robotic system with advanced features that is comparable to an autonomous fire rescue truck. \n",
    "\n",
    "In the initial setup, a webcam captures the surroundings of the experimental area. Real-time processing using traditional image techniques extracts essential map details like the robot's position, the map itself, fixed obstacles, and the destination. The A* algorithm then calculates the best route, relaying instructions to the global controller of the Thymio robot. This controller, in turn, directs the motors to follow the optimal path. If the Thymio detects an obstacle ahead through its proximity sensors, local navigation takes charge, guiding the robot to avoid collisions. Additionally, even if the Thymio is moved unexpectedly, it can still autonomously navigate its way to the intended destination.\n",
    "\n",
    "Our project's story is based on a simulated scenario in which an autonomous fire rescue robot sets out on a mission to locate and rescue people trapped in a burning building, taking inspiration from the urgent and essential nature of firefighting scenarios. This hypothetical scenario highlights the necessity for robotic systems to manoeuvre through dangerous situations, assess the situation, and carry out precision rescue operations. It also matches the urgency and complexity of actual firefighting operations where speed and efficiency are keys for a successful salvation of the endangered victims.\n",
    "\n",
    "### **Environment**\n",
    "\n",
    "The operational canvas of our robot unfolds across a dynamic terrain represented by a grid-type map. Comprising 49 squares (7 rows and 7 columns), the map features white pathways and black barriers. ArUco markers strategically designate corners and the robot's position. The goal and the obstacles are respectively described by a blue square and a red square. \n",
    "\n",
    "</div style=\"width: 30%;\">\n",
    "    <img src=\"./map.jpeg\" />\n",
    "    <style=\"width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Figure 1: Map setup </p>\n",
    "</div>\n",
    "\n",
    "This environment facilitates the interplay of the vision module and the Thymio robot, allowing it to follow the global navigation path and dynamically adapt to obstacles using local navigation. A Kalman filter implementation further augments precision, incorporating equations for x and y coordinates, Thymio's angle, and motor velocities.\n",
    "\n",
    "### **Structure**\n",
    "\n",
    "In our project's organisational structure, a central \"Command Centre\" assumes a pivotal role as the hub for coordinating the cooperation of many modules, ensuring the seamless functioning of our self-contained fire rescue robot. This crucial module takes on the tasks of establishing the Thymio connection, carrying out careful camera calibration to maximise colour and edge detection, calculating the grid map, and drawing the route for navigation. This command centre is modular in operation and handles critical robot duties such as taking quick decisions regarding the presence and absence of an obstacle. The robot operates in two different modes: local navigation, which allows for real-time obstacle avoidance, and global navigation, which plots a route to the closest checkpoint. While maintaining knowledge of the position and orientation of the robot using either the Camera module or the Kalman filter. The diagram presented below illustrates the overarching structure of our project.\n",
    "\n",
    "</div style=\"width: 30%;\">\n",
    "    <img src=\"./general_diagram.jpeg\" />\n",
    "    <style=\"width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Figure 2: General Diagram of the overall structure of the project</p>\n",
    "</div>\n",
    "\n",
    "\n",
    "The main variables : \n",
    "\n",
    "grid :\n",
    "obs :\n",
    "obs_grid :\n",
    "targets :\n",
    "goal :\n",
    "width : \n",
    "height :\n",
    "pos : \n",
    "angle : \n",
    "path : \n",
    "visitedNodes :\n",
    "vector_to_checkpoint : \n",
    "### **Modules**\n",
    "\n",
    "#### **Robot State Estimation with Extended Kalman Filter**\n",
    "\n",
    "In our autonomous ambulance rescue robot project, accurate position and speed estimation are vital for effective navigation. We utilize an Extended Kalman Filter (EKF) implemented in the `KalmanFilter` class. The state space representation is tailored to our specific needs, considering a 4-dimensional state vector \\([ \\text{x}, \\text{y}, \\text{v}_x, \\text{v}_y ]\\), where \\(\\text{v}_x\\) and \\(\\text{v}_y\\) represent the linear velocities in the x and y directions, respectively.\n",
    "\n",
    "\n",
    "##### **Coordinate System**\n",
    "\n",
    "The coordinate system follows the convention shown below.\n",
    "\n",
    "Here, \\(x\\) and \\(y\\) are in millimeters, and \\(\\text{{vx}}, \\text{{vy}}\\) are in millimeters per second.\n",
    "\n",
    "##### **Discrete Model**\n",
    "\n",
    "The discrete motion model for the Kalman filter is given by:\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\mathbf{z}_{k+1} &= f(\\mathbf{z}_k) \\\\\n",
    "\\mathbf{z}^+ &= \\begin{bmatrix} x + \\frac{v_r + v_l}{2} \\cos(\\theta) T_s \\\\ y - \\frac{v_r + v_l}{2} \\sin(\\theta) T_s \\\\ \\text{{vx}} \\\\ \\text{{vy}} \\end{bmatrix} \n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "Here, \\(k_s\\) is the constant factor, and \\(T_s\\) is the time between two iterations. The covariance matrix \\(\\mathbf{Q}\\) represents the motion model's uncertainty.\n",
    "\n",
    "##### **Measurement Model**\n",
    "\n",
    "The discrete measurement model is given by:\n",
    "\n",
    "\\[\n",
    "\\begin{align*}\n",
    "\\mathbf{z}_{k+1} &= h(\\mathbf{z}_k) \\\\\n",
    "\\mathbf{z}^+ &= \\begin{bmatrix} x_{\\text{{measured}}} \\\\ y_{\\text{{measured}}} \\\\ v_{r \\text{{ measured}}} \\\\ v_{l \\text{{ measured}}} \\end{bmatrix} + \\mathbf{v}\n",
    "\\end{align*}\n",
    "\\]\n",
    "\n",
    "The measurement model incorporates data from the camera and the motor encoders, providing measured values for position and motor speeds. The covariance matrix \\(\\mathbf{R}\\) captures the measurement noise.\n",
    "\n",
    "##### **Extended Kalman Filter Algorithm**\n",
    "\n",
    "The steps of the extended Kalman filter algorithm, tailored to our implementation, are as follows:\n",
    "\n",
    "1. **Prediction Step:**\n",
    "In this step, the Kalman filter predicts the next state of the system based on the previous state estimate and the system dynamics, incorporating the process model. The predicted state and covariance represent the system's expected behavior, considering motion and process uncertainties :\n",
    "\n",
    "   \\begin{align*}\n",
    "   \\hat{\\mathbf{x}}_{k \\mid k-1} &= f(\\hat{\\mathbf{x}}_{k-1 \\mid k-1}, \\mathbf{u}_k) \\\\\n",
    "   \\mathbf{P}_{k \\mid k-1} &= \\mathbf{F}_k \\mathbf{P}_{k-1 \\mid k-1} \\mathbf{F}_k^T + \\mathbf{Q}_k\n",
    "   \\end{align*}\n",
    "\n",
    "2. **Measurement Residual:**\n",
    "The measurement residual calculates the difference between the predicted measurement (obtained from the predicted state) and the actual measurement. It represents the discrepancy or error between the expected and observed values, capturing the system's deviation from the predicted state : \n",
    "\n",
    "   \\begin{align*}\n",
    "   \\tilde{\\mathbf{y}}_k &= \\mathbf{z}_k - h(\\hat{\\mathbf{x}}_{k \\mid k-1})\n",
    "   \\end{align*}\n",
    "\n",
    "3. **Measurement Update:**\n",
    "In the measurement update step, the Kalman filter refines its state estimate by incorporating the measured values. The Kalman gain is calculated to give more weight to reliable measurements and less weight to uncertain ones. The updated state estimate and covariance reflect a more accurate representation of the system's true state, considering both prediction and measurement inform : \n",
    "\n",
    "   \\begin{align*}\n",
    "   \\mathbf{S}_k &= \\mathbf{H}_k \\mathbf{P}_{k \\mid k-1} \\mathbf{H}_k^T + \\mathbf{R}_k \\\\\n",
    "   \\mathbf{K}_k &= \\mathbf{P}_{k \\mid k-1} \\mathbf{H}_k^T (\\mathbf{S}_k)^{-1} \\\\\n",
    "   \\hat{\\mathbf{x}}_{k \\mid k} &= \\hat{\\mathbf{x}}_{k \\mid k-1} + \\mathbf{K}_k \\tilde{\\mathbf{y}}_k \\\\\n",
    "   \\mathbf{P}_{k \\mid k} &= (\\mathbf{I} - \\mathbf{K}_k \\mathbf{H}_k) \\mathbf{P}_{k \\mid k-1}\n",
    "   \\end{align*}\n",
    "\n",
    "\n",
    "##### **Estimation of Ts (Time difference)**\n",
    "\n",
    "The accurate estimation of the time between iterations (\\(T_s\\)) is essential for the dynamic performance of the Kalman filter, directly influencing temporal alignment, motion prediction, and covariance matrix updates. A precise \\(T_s\\) ensures that predictions align with the system's temporal dynamics, maintaining the filter's adaptability to dynamic changes. Inaccuracies in \\(T_s\\) can disrupt temporal alignment, affecting the filter's ability to provide accurate state estimates and impacting control actions.\n",
    "\n",
    "##### **Estimation of Noise Matrices**\n",
    "\n",
    "For the motion model:<br />\n",
    "$$\n",
    "Q=\\begin{bmatrix}\n",
    "0.04 & 0 & 0 & 0\\\\\n",
    "0 & 0.04 & 0 & 0\\\\\n",
    "0 & 0 & 6 & 0\\\\\n",
    "0 & 0 & 0 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "We derived the values of 0.04 and the angle through empirical estimation, without a specific deterministic basis. Regarding the speed, empirical data obtained during the eighth week informed our estimation process, wherein half of the variance was attributed to the inherent motion dynamics, while the remaining half originated from measurement uncertainties. <br>\n",
    "\n",
    "For the measurement model:<br />\n",
    "$$\n",
    "R=\\begin{bmatrix}\n",
    "0.25 & 0 & 0 & 0\\\\\n",
    "0 & 0.25 & 0 & 0\\\\\n",
    "0 & 0 &6 & 0\\\\\n",
    "0 & 0 & 0 & 6\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "These values are estimated, with half of the variance attributed to motion and the other half to measurement.\n",
    "\n",
    "\n",
    "## 2. Vision Module<a name=\"vision-module\"></a>\n",
    "\n",
    "### 2.1 Overview<a name=\"overview\"></a>\n",
    "\n",
    "The vision module serves as the visual intelligence of the Thymio robot, forming a critical component within the broader project framework. Its primary purpose revolves around interpreting the surrounding environment through a predefined grid, feature extraction, and the recognition of specific visual cues, such as QR codes. The overarching objectives of this module are multi-faceted and include determining the robot's precise position, identifying obstacles marked in red, pinpointing the goal represented in blue, and detecting a secondary goal highlighted in green.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from math import floor\n",
    "from enum import Enum\n",
    "from vision.utils import print_error\n",
    "from vision.constants import grid_height_cells, grid_width_cells\n",
    "```\n",
    "\n",
    "```python\n",
    "    class VisionObjectsTypes(Enum):\n",
    "        GENERAL = 0\n",
    "        OBSTACLE = 1\n",
    "        GOAL = 2\n",
    "        TARGET = 3\n",
    "```\n",
    "\n",
    "The vision module starts by importing necessary libraries for image processing, numerical operations, and custom utility functions and constants needed for this project. The enumeration class presented above defines different types of vision objects: general, obstacle, goal, and target.\n",
    "\n",
    "### 2.2 Reading the Predefined Grid<a name=\"reading-the-predefined-grid\"></a>\n",
    "\n",
    "The core functionality of the vision module lies in its ability to comprehend a predefined grid. This involves processing visual data to interpret the grid's layout, which serves as the navigational reference for the Thymio robot. By parsing the grid, the vision module equips the robot with essential spatial awareness, enabling it to make informed decisions about its movements and thus allowing more complex control of the thymio such as the A* global path computation or the P controller for the robot's movements.\n",
    "\n",
    "### 2.3 Feature Extraction<a name=\"feature-extraction\"></a>\n",
    "\n",
    "To navigate effectively, the vision module engages in feature extraction, isolating distinctive elements within the visual input. These features are in general the edges, corners and the unique patterns of the QR codes that aid in recognizing the environment. By extracting these features, we gain a more detailed understanding the surroundings and utilising this to precise localization and navigation.\n",
    "\n",
    "### 2.4 QR Code Identification<a name=\"qr-code-identification\"></a>\n",
    "\n",
    "The vision module is designed to identify QR codes, leveraging their encoded information for various purposes within the project such as conveying data regarding the Thymio's position, orientation and the Grid's corners.\n",
    "\n",
    "### 2.5 Objectives<a name=\"objectives\"></a>\n",
    "\n",
    "#### 2.5.1 Determining the Robot's Position<a name=\"determining-the-robots-position\"></a>\n",
    "\n",
    "A fundamental objective of the vision module is to accurately determine the Thymio robot's position within its environment. This involves integrating information from the predefined grid, extracted features, and QR codes to establish a real-time understanding of the robot's spatial coordinates, speed and orienation.\n",
    "\n",
    "#### 2.5.2 Recognizing Obstacles (In Red)<a name=\"recognizing-obstacles\"></a>\n",
    "\n",
    "The vision module is tasked with identifying obstacles marked in red. This capability enables the Thymio robot to proactively detect and respond to potential hindrances in its path, facilitating efficient and safe navigation. Using the camera to locate the predefined obstacles in red allows the robot to only detect any new detected obstacle in it's path.\n",
    "\n",
    "#### 2.5.3 Identifying the Goal (In Blue)<a name=\"identifying-the-goal\"></a>\n",
    "\n",
    "Recognizing the goal, indicated in blue, is important for the robot's navigation strategy. The vision module's ability to identify the goal allows the Thymio robot to plan and execute optimal paths using algorithms (in our case the __A*__), steering towards final destinations.\n",
    "\n",
    "In essence, the vision module serves as the \"eyes\" of the Thymio robot, enabling it to perceive, interpret, and respond intelligently to the visual cues present in its environment. The successful execution of these vision objectives forms the foundation for the robot's overall navigation and mission accomplishment.\n",
    "\n",
    "### 2.6 Pre-processing for Vision<a name=\"pre-processing\"></a>\n",
    "\n",
    "The original frames captured by the camera undergo pre-processing to address issues like chromatic aberration, noise, and unstandardization caused by changes in camera position and shooting angle. The map region is extracted using markers, and perspective projection is removed with a projective transform for robustness. The image is converted to grayscale, filtered for noise with a Gaussian filter, and thresholded for binary mapping. Future improvements may include exploring different filters or thresholding methods to enhance performance under varied conditions.\n",
    "\n",
    "### 2.7 Mapping and Localization<a name=\"mapping-and-localization\"></a>\n",
    "\n",
    "After pre-processing, the image is cropped into grids, and the vision module identifies walls and goals using ArUco markers. The robot's position and rotation gained by the vision module are used in conjunction with motor speed estimates for a more precise state estimation. This information is then utilized in control and navigation modules, providing a comprehensive understanding of the robot's real-time state.\n",
    "\n",
    "Certainly! Here is the modified section, including the provided assumptions, integrated into the local navigation part:\n",
    "\n",
    "\n",
    "## 3. Optimal Path Computation<a name=\"optimal-path-computation\"></a>\n",
    "\n",
    "### 3.1 Pathfinding Precision<a name=\"pathfinding-precision\"></a>\n",
    "\n",
    "This section will be focusing on global navigation, unraveling the choices of the Thymio's optimal path computation facilitated by the __A* algorithm__. Beyond mere movement, this algorithmic approach aspires to chart the most efficient course for the robot, navigating through a predefined grid with discernment minimising the cost needed to achieve the desired result.\n",
    "\n",
    "### 3.2 A* Algorithmic Mastery<a name=\"a-algorithmic-mastery\"></a>\n",
    "\n",
    "\n",
    "#### 3.3 Navigational Options<a name=\"navigational-options\"></a>\n",
    "Two distinct sets of movement options are already provided for the Thymio's trajectory within the A* algorithm. Both 4-connectivity and 8-connectivity movements are optimally computed. After careful consideration, we have chosen to use the 4N connectivity for our project. This decision is informed by the nature of the environment, which tries to mirrors a realistic scenario with streets and buildings where diagonal movements are restricted. The adoption of 4N connectivity aligns with the practical constraints of the project, ensuring computational efficiency while navigating through the constrained layout of the simulated environment.\n",
    "\n",
    "\n",
    "#### 3.4 Path Reconstruction Elegance<a name=\"path-reconstruction-elegance\"></a>\n",
    "\n",
    "The reconstruction of the path is important in how we make the Thymio move. This way of doing things in the __A* algorithm's__ code ensures that we can efficiently figure out the best path for the Thymio to follow. It's like setting up a smooth roadmap that helps the Thymio navigate carefully and reach its destination.\n",
    "\n",
    "## 4. Motion Control Implementation Overview<a name=\"motion-control-implementation-overview\"></a>\n",
    "\n",
    "### 4.1 Initialization and Configuration<a name=\"initialization-and-configuration\"></a>\n",
    "\n",
    "The motion control process initiates with the instantiation of the `Control` class, where parameters for motion control, including the proportional (kp), integral (ki), and derivative (kd) gains, as well as the robot's wheel base width, are defined. These parameters establish the foundation for subsequent motion control strategies. Additionally, the class contains methods for sensor data retrieval and actuation, contributing to the overall adaptability of the robot's motion.\n",
    "\n",
    "### 4.2 Sensor Data Retrieval and Actuation<a name=\"sensor-data-retrieval-and-actuation\"></a>\n",
    "\n",
    "The `Control` class features a method named `get_sensors` responsible for retrieving sensor data such as motor speeds and horizontal proximity values. This method provides essential information for making informed decisions during the motion control process and also for the local avoidance tasks. The `set_motors_PID` method is implemented to adjust the left and right motor speeds based on PID control, enhancing the adaptability and responsiveness of the robot to environmental changes and thus limiting the angular error allowed for the robot.\n",
    "\n",
    "### 4.3 PID Controller Implementation<a name=\"pid-controller-implementation\"></a>\n",
    "\n",
    "A PID controller is implemented within the `pid_controller` function, utilizing proportional, integral, and derivative terms to dynamically adjust motor speeds. The control gains (kp, ki, kd) and the robot's width play a crucial role in determining the correction applied to the motors. The function also includes a mechanism to handle aggressive corrections for significant angle deviations, ensuring effective turning when needed. The `normalize_angle` function is utilized to maintain the angle within the range of -180 to 180 degrees.\n",
    "\n",
    "**Table of Constants for Motion Control:**\n",
    "\n",
    "| Constant        | Value/Description                          |\n",
    "|-----------------|--------------------------------------------|\n",
    "| kp              | 2.0                                        |\n",
    "| ki              | 0.04                                       |\n",
    "| kd              | 0.01                                       |\n",
    "| robot_width     | 0.1                                        |\n",
    "\n",
    "This set of constants is meticulously chosen to balance the responsiveness and stability of the robot's motion control, ensuring effective navigation in various scenarios. The proportional, integral, and derivative gains are fine-tuned to achieve optimal performance, while the robot's width is considered for accurate motion calculations.\n",
    "\n",
    "## 5. Local Navigation Strategy<a name=\"local-navigation-strategy\"></a>\n",
    "\n",
    "</div style=\"width: 30%;\">\n",
    "    <img src=\"./Local_nav_model.jpeg\" />\n",
    "    <style=\"width: 100%;\">\n",
    "    <p style=\"text-align: center;\">Figure 3: Local Navigation System Operation Diagram</p>\n",
    "</div>\n",
    "\n",
    "The Local Navigation module is designed to respond dynamically to obstacles detected by the Thymio robot's proximity sensors. The goal is to trigger Local Navigation when an obstacle is detected, ensuring the robot can navigate around it effectively. Proximity sensors' captured values trigger Local Navigation when above a desired threshold distance.\n",
    "\n",
    "#### 5.1 Assumptions<a name=\"local-assumptions\"></a>\n",
    "\n",
    "In our environment, some obstacles are strategically placed along the robot's path. The Local Navigation module operates under the following assumptions:\n",
    "\n",
    "1. **Obstacle Blocking Assumption:**\n",
    "   - The obstacles are completely blocking the robot's path. This choice aligns with our decision to perform Global Navigation only once at the program's beginning. This assumption allows Local Navigation to focus on avoiding specific obstacles without being constrained by the assumptions made during Global Navigation and thus releaving us from the necessity of recomputing the global path\n",
    "\n",
    "2. **Light Reflective Obstacle Assumption:**\n",
    "   - The obstacles are assumed to be light-reflective as the proximity sensors utilize infrared light. The sensors consist of a transmitter and a receiver, with the light reflecting off the obstacle. This assumption rules out obstacles that diffract light extensively, such as glass, ensuring reliable detection.\n",
    "\n",
    "3. **Simple Convex Geometrical Section Assumption:**\n",
    "   - Obstacles are assumed to have a simple convex geometric shape including triangles, rectangles and squares. This assumption facilitates the robot's ability to navigate around obstacles by following specific scenarios preimplemented. It ensures that the robot can circumvent obstacles without encountering complex concave shapes that might hinder navigation.\n",
    "\n",
    "#### 5.2 Operational Assumptions<a name=\"local-operational-assumptions\"></a>\n",
    "\n",
    "Given these assumptions, the Local Navigation module specifically assumes that obstacles are square-shaped. When an obstacle is detected, the entire square is considered impassable, and the robot plans its path accordingly. The assumption further specifies that the obstacle is positioned centrally within the square.\n",
    "\n",
    "This operational approach enables the robot to navigate around square obstacles effectively. However, it is important to note that the Local Navigation module does not require adjustements if different obstacle shapes are introduced due to the designed library of local paths to follow. The successful execution of Local Navigation relies on the accurate adherence to these assumptions for obstacle characteristics and the geometric layout of the environment.\n",
    "\n",
    "### 5.3 Obstacle Detection and Handling<a name=\"obstacle-detection-and-handling\"></a>\n",
    "\n",
    "\n",
    "**Obstacle Identification:** \n",
    "The robot retrieves and scales data from the proximity sensor to identify new obstacles in its path that may not have been captured by the camera. The `local_nav()` function checks if the proximity sensor in front of the robot, denoted as `x[2]`, does not detect a new obstacle. Upon the threshold of 20 being exceeded by the value of `x[2]`, the system switches to the obstacle-handling state, triggering the `obstacle_function()` when the boolean variable `obstacle`is set to true.\n",
    "\n",
    "**Obstacle Handling:**\n",
    " When an obstacle is encountered, the robot stops (`forward_robot(0)`) and assesses the position relative to the global path. Three scenarios are considered:\n",
    "  - If the global path is behind the obstacle, the robot maneuvers to reach the path (`state = obstacle_behind`).\n",
    "  - If the target is to the left of the obstacle, the robot executes a specific maneuver (`state = obstacle_left`).\n",
    "  - If the target is to the right of the obstacle, the robot follows a different maneuver (`state = obstacle_right`). \n",
    "To determine the scenario, we conduct a comparison between the robot's position in front of the obstacle (`position[0]`)]) and the subsequent position in the global path (`path[0][0]`), taking into consideration the abscissae of the positions.\n",
    "\n",
    "\n",
    "### 5.4 Maneuvering Strategies<a name=\"maneuvering-strategies\"></a>\n",
    "\n",
    "Depending on the selected senario, we will apply different maneuvering strategies. When an obstacle is encountered, the robot dynamically decides whether to turn left or right based on its initial target position relative to the obstacle. This decision influences the choice of one of the following functions. This approach enables the robot to adapt its maneuvering strategy, ensuring efficient obstacle avoidance and alignment with the next best location of the global path :\n",
    "\n",
    "- **Maneuvering Behind Obstacle (`obstacle_behind`):** \n",
    "    ```python\n",
    "        handle_obstacle_behind(path, special_cases)\n",
    "    ```\n",
    "- **Maneuvering Left of Obstacle (`obstacle_left`):**\n",
    "    ```python\n",
    "        handle_obstacle_left(position, obstacle_position)\n",
    "    ```\n",
    "- **Maneuvering Right of Obstacle (`obstacle_right`):**\n",
    "    ```python\n",
    "        handle_obstacle_right(position, obstacle_position)\n",
    "    ```\n",
    "When the target is situated behind an obstacle, two scenarios merit consideration:\n",
    "\n",
    "- If the target is behind the obstacle, and the subsequent step in the global path does not align with the predefined special cases, the robot straightforwardly moves to the position behind the obstacle.\n",
    "- If the target is behind the obstacle, and the next step in the global path corresponds to either of the two special cases previously defined (refer to the sketch above), it is more optimal for the robot to navigate directly to the second position in the global path, precisely on the special cases.\n",
    "\n",
    "Finally, once the obstacle is navigated, we enter in `state = orientation`. The robot assesses the next step in the global path to adjust its orientation with the next step in the path.To decide if it has to turn or not we just compare the abscisse of the robot and the one of the next position in the path.\n",
    "\n",
    "- **Next Step (`next_step`):** \n",
    "    ```python\n",
    "       handle_target_state(path, k)\n",
    "    ```\n",
    "\n",
    "## 6. Kalman Filter in Localization<a name=\"kalman-filter-in-localization\"></a>\n",
    "\n",
    "### 6.1 Adaptive Position Estimation<a name=\"adaptive-position-estimation\"></a>\n",
    "\n",
    "The Kalman Filter is a pivotal component in our localization strategy, adapting to changing conditions and continuously refining its estimates based on motor speed inputs. We use the kalman mainly in the case of camera obscurity or perturbation to send the estimated position and angle of the robot to the control functions in real time. The Kalman is structures as follows : first the functoin called **compute_x_y_speed(self, left_motor_speed, right_motor_speed)** is called taking as inputs the left and right motor speeds, it then computes the speeds in term of linear speed (v_r - v_l)/2 , the angular speed, the the vectorial speeds [v_x and v_y] and finally uses the difference between the 2 wheels' speed to compute the angular variation $\\Delta\\theta$ to estimate the actual orienation and angle of the our Robot.\n",
    "\n",
    "In our state space representation, we chose not to include the angle (\\(\\theta\\)) as a primary state variable to enhance computational efficiency. Instead, we compute and estimate \\(\\theta\\) separately using the motor inputs from sensor values. This modular approach streamlines computations, reduces complexity, and aligns with the specific requirements of our application. In addition, the speeds returned from the functions are then being passed through our extended Kalman Filter to ultimately conclude our estimation for the position which is the most important element for which we have the estimator. In addition to our global choices, we have decided to facilitate the readability of the filtering task and merge both steps explained above for this task : the **prediction** and the **update** in a single predict function that outputs the expected status of the system and the updated matrix **P** as well.\n",
    "\n",
    "#### Constants used for the filtering \n",
    "\n",
    "| Constant                   | Value or Description            | Justification                                                                |\n",
    "|-----------------------------|--------------------------------|------------------------------------------------------------------------------|\n",
    "| `dt`                        | 0.08                                          | Time step for the state transition matrix by default untill the comuted time is passed to the function                    |\n",
    "| `speed_conv_factor`         | 0.35                                           |Conversion factor for motor speed to linear speed computed by observing the robot for a distance of 12cm with a speed ot 80 and computing the \\(\\delta t\\) thus extracting the factor             |\n",
    "| `wheel_width`               | 100 (distance between the two wheels in mm)    | Distance between the two wheels of the robot used to compute the angular speed of the robot             |\n",
    "\n",
    "\n",
    "```python\n",
    "class KalmanFilter(object):\n",
    "    def __init__(self, dim_x=4, dim_z=2, dt=0.1):\n",
    "        # Initialization code\n",
    "\n",
    "    def compute_x_y_speed(self, left_motor_speed, right_motor_speed):\n",
    "        # Compute linear speed and angular speed\n",
    "        return speed, new_angle\n",
    "    \n",
    "    def predict(self, left_motor_speed, right_motor_speed, orientation, x_est_prev, P_est_prev):\n",
    "        # Measurement prediction covariance\n",
    "        S = np.dot(self.H, np.dot(self.P_est, self.H.T)) + self.R\n",
    "\n",
    "        # Kalman gain\n",
    "        K = np.dot(self.P_est, np.dot(self.H.T, np.linalg.inv(S)))\n",
    "\n",
    "        # A posteriori estimate\n",
    "        self.x_est = self.x_est + np.dot(K, i)\n",
    "        self.P_est = self.P_est - np.dot(K, np.dot(self.H, self.P_est))\n",
    "\n",
    "        # Extracting relevant information for the output...\n",
    "        return estimated_position, estimated_speed, angle, self.x_est, self.P_est\n",
    "```\n",
    "\n",
    "1. **`__init__(self, dim_x=4, dim_z=2, dt=0.1):`**\n",
    "   - **Purpose:** Initializes the Kalman filter object with specified dimensions and time parameters.\n",
    "\n",
    "2. **`compute_x_y_speed(self, left_motor_speed, right_motor_speed):`**\n",
    "   - **Purpose:** Computes the linear speed (in x and y directions) and angular speed of the robot based on left and right motor speeds.\n",
    "   - **Returns:**\n",
    "     - `speed`: Linear speed of the robot.\n",
    "     - `new_angle`: The robot's new estimated angle.\n",
    "\n",
    "3. **`predict(self, left_motor_speed, right_motor_speed, orientation, x_est_prev, P_est_prev):`**\n",
    "   - **Purpose:** Predicts the next state estimate using the Kalman filter based on motor speeds, orientation, and previous estimates.\n",
    "   - **Returns:**\n",
    "     - `estimated_position`: Estimated position of the robot.\n",
    "     - `estimated_speed`: Estimated linear speed of the robot.\n",
    "     - `angle`: Estimated angular speed of the robot.\n",
    "     - `self.x_est`: Updated state estimate.\n",
    "     - `self.P_est`: Updated state covariance estimate.\n",
    "\n",
    "### 6.2 Compensating for Uncertainties<a name=\"compensating-for-uncertainties\"></a>\n",
    "\n",
    "To enhance the robustness of our localization system, we explicitly consider uncertainties in motor speed measurements and orientation through the process uncertainty matrix (`Q`) and measurement uncertainty matrix (`R`). The values computed in the code are either measured experimentally or extracted from the Practical session number 8 of the course of Mobie Robotics with Professor Francesco Mondada.\n",
    "\n",
    "### **7 Conclusion**\n",
    "\n",
    "We methodically created and included key components in this robotics project to imitate the operations of an autonomous fire rescue robot. The robot can determine its position, identify obstacles, and find goals thanks to the Vision Module's interpretation of a specified grid, feature extraction, and QR code recognition. The A* algorithm drives the Optimal Path Computation, which accurately determines the best path through a grid for the robot's precision navigation. The robot's movements are dynamically adjusted by the Motion Control Implementation, and obstructions detected by proximity sensors are skillfully responded to by the Local Navigation Strategy. In localization, the Kalman Filter improves position estimations by accounting for uncertainties. This study shows how an autonomous fire rescue robot can successfully navigate complex areas, avoid obstacles, and accomplish mission objectives.\n",
    "\n",
    "### **Main Loop**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
